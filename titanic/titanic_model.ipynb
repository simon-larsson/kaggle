{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Passanger Survival Prediction\n",
    "\n",
    "This notebook contains my efforts and thoughts for the Titanic: Machine Learning from Disaster [Kaggle](https://www.kaggle.com/c/titanic) competition. The aim is to explore different machine learning concepts rather than to find the optimal score. It is also a practice in making a well documented and easy to follow notebook.\n",
    "\n",
    "### Covered topics\n",
    "- Data preprocessing using Pandas\n",
    "- Feature engineering\n",
    "- Conventional ML models using scikit-learn\n",
    "- Ensambles\n",
    "- Hyperparameter tuning\n",
    "- Neural network model using Tensorflow\n",
    "\n",
    "### Things for another day\n",
    "- Visualizations\n",
    "- Regularization\n",
    "- Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import math\n",
    "from time import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removes a warning in sklearn that will be fixed during an update mid 2018\n",
    "import warnings\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "    le = sk.preprocessing.LabelEncoder()\n",
    "    le.fit([1, 2, 2, 6])\n",
    "    le.transform([1, 1, 2, 6])\n",
    "    le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "We will use widely used [Pandas](https://pandas.pydata.org/) to preprocess our data. We start with reading the data from .csv with read_csv(). When we have our dataframe in memory we can use head to take a first look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/GitHub/kaggle/titanic/data/train.csv', sep=',', header=0)\n",
    "print('Data size: ' + str(df.shape))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the data\n",
    "Now we can inspect our features. We have 11 feauters, which we will call X, and 1 output (Survived), which we will call y. We can also see that we have 891 training examples. Now let's go over our features and try to reason over them. The most important thing is to find data that we think might affect the chance of survival for a passenger.\n",
    "\n",
    "#### PassangerId\n",
    "This is just used for indentification when scoring the predictions. It does not contain any ground truth about passengers and their survival and should therefore be removed before training.\n",
    "\n",
    "#### Survived\n",
    "This is what we want to predict, our output y. It should be split from the features and kept separate.\n",
    "\n",
    "#### Pclass\n",
    "The fare class of the passenger. Pclass = 1 is first class, Pclass = 2 is second class and Pclass = 3 is third class. This feature will likely affect survival. Things such people density and the distribution of [life boats](https://en.wikipedia.org/wiki/Lifeboats_of_the_RMS_Titanic) will likely vary amoung classes and play a part in survival.\n",
    "\n",
    "#### Name\n",
    "There are two things with the name that might be interesting in terms of surivival. The name might give us the gender of the passenger. We can however see that the gender is already provided in another feature, so there is no point in extracting it here. The second, and more interesting thing, is the title. The title can tell us the marital and sociatal status of a passenger.\n",
    "\n",
    "#### Sex\n",
    "[\"Women and children first\"](https://en.wikipedia.org/wiki/Women_and_children_first) is famously associated with the Titanic. Therefore it is safe to assume that sex will play a large factor in survival.\n",
    "\n",
    "#### Age\n",
    "The same motivation as with sex can be applied as to why age would be important for survival.\n",
    "\n",
    "#### SibSp\n",
    "Number of siblings/spouses onboard. Probably will be a factor for survival since family members can help eachother.\n",
    "\n",
    "#### Parch\n",
    "Number of parents/children onboard. Very similar to the feature above.\n",
    "\n",
    "#### Ticket\n",
    "The tickets numbers seem to have some useful information in them. However, there seem to be little structure in the data. I decided to skip it to minimize initial effort. Might revise it later.\n",
    "\n",
    "#### Fare\n",
    "The price of the ticket. Will likely have similar factors as Pclass.\n",
    "\n",
    "#### Embarked\n",
    "Port of embarkation. Where C = Cherbourg, Q = Queenstown and S = Southampton. This should, at first glance, not play a big role in survival. However, it could possibly be a indirect factor and should not be dismissed without investigation.\n",
    "\n",
    "### Data size\n",
    "891 samples of data is on the low end for machine learning. It will mean that the model might be subseptible to overfitting. Overfitting is when to model specializes too much on the given dataset instead of learning a ground truth which that can help with predicting previously unseen data. It also means that it might be reasonable to chose simpler, conventional, machine learning models instead of the more data hungry neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "Below is just some util functions that will later be used for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checks and returns if a string contains one of a list of given substrings.\n",
    "def substrings_in_string(whole, subs): \n",
    "    \n",
    "    for x in subs:\n",
    "        if x in str(whole): \n",
    "            return x\n",
    "        \n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract title from a name\n",
    "def extract_title(full_name):\n",
    "    \n",
    "    full_name  = str(full_name)\n",
    "    \n",
    "    x = full_name.split(\", \")\n",
    "    x = x[1]\n",
    "    x = x.split('.')\n",
    "    title = x[0]\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplify titles into fewer more significant titles\n",
    "def simplify_titles(x):\n",
    "    \n",
    "    title=x['Title']\n",
    "    \n",
    "    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col', 'Sir']:\n",
    "        return 'Mr'\n",
    "    elif title in ['the Countess', 'Mme', 'Dona', 'Lady']:\n",
    "        return 'Mrs'\n",
    "    elif title in ['Mlle', 'Ms']:\n",
    "        return 'Miss'\n",
    "    elif title == 'Dr':\n",
    "        if x['Sex'] == 0:\n",
    "            return 'Mrs'\n",
    "        else:\n",
    "            return 'Mr'\n",
    "    else:\n",
    "        return title\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and feature engineering\n",
    "Data preprocessing and feature engineering go hand in hand and have the aim find the best possible data for a model. They are together probably the most important part for the success of a model, especially when the data size is small.\n",
    "\n",
    "### Data preprocessing\n",
    "Data preprocessing is when data is made adjusted to be recieved by a model. The following processing is performed:\n",
    "- Removing noise - Data not relevant for the prediction will make the model more noisy and should be removed.\n",
    "- Missing values - Values that are missing are filled with mean of most common occuring value.\n",
    "- Normalization - If data on different scales will serve as initial weights for the model. Therefor it is good to normalize the data and let the model find the weights be itself. A good normalization is to have all data on a magnititude of 0.0 - 1.0.\n",
    "- Data discretation - Data that represents different categories (such as Pclass) can be represented as separate discreet features instead of quanties.\n",
    "\n",
    "### Feature engineering\n",
    "Feature engineering is when domain knowledge is applied to find new features that will be more useful for the model. The following techniques were used.\n",
    "- Indicator Variables - Quantities can be divided into meaningful brackets. Example: Age can be divided into categories to find children since we know that they are more likely to live.\n",
    "- Interaction Features - Features can sometimes be combined into something meaningful. Example: SibSp and Parch can be combined to find the family size.\n",
    "- Grouping sparse data - Categorical data that has low occurence can be grouped into larger meaningful categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, prediction_data=False, print_info=False):\n",
    "    # Age has missing values which is replaced with average\n",
    "    # Might also consider dividing age into classes of age brackets\n",
    "    df['Age'].fillna((df['Age'].mean()), inplace = True)\n",
    "    df['Fare'].fillna((df['Fare'].mean()), inplace = True)\n",
    "    \n",
    "    # Encode sex into binary (0 = male, 1 = female)\n",
    "    df['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n",
    "    \n",
    "    # Use the prefix in cabin to find the deck of the cabin\n",
    "    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n",
    "    df['Deck' ]= df['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "    \n",
    "    # Calculate family size by combining SibSp and Parch\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "    \n",
    "    # Divide age into useful brackets\n",
    "    df['Child'] = df['Age'].apply(lambda x: 1 if x <= 12 else 0)\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract and process titles\n",
    "    df['Title']= df['Name'].map(lambda x: extract_title(x))\n",
    "    if not prediction_data:\n",
    "        unique_titles = df['Title'].unique()\n",
    "        survival_by_title = df.groupby('Title').mean()['Survived']\n",
    "        \n",
    "    df['Title']=df.apply(simplify_titles, axis=1)\n",
    "    if not prediction_data:\n",
    "        unique_titles_simplified = df['Title'].unique()\n",
    "        survival_by_title_simplified = df.groupby('Title').mean()['Survived']\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Gather info on the significance of these classes for survival\n",
    "    # Class was, as expected, significance for surival with the rates (1st - 63%, 2nd - 47%, 3rd - 24%)\n",
    "    # Embarked was suprisingly significant, C - Cherbourg had 55% surivial rate when the mean was just 38%\n",
    "    if not prediction_data:\n",
    "        survival_by_plcass = df.groupby('Pclass').mean()['Survived']\n",
    "        survival_by_deck = df.groupby('Deck').mean()['Survived']\n",
    "        survival_by_embark = df.groupby('Embarked').mean()['Survived']\n",
    "        survival_by_familysize = df.groupby('FamilySize').mean()['Survived']\n",
    "        survival_by_child = df.groupby('Child').mean()['Survived']\n",
    "    \n",
    "    # Split classes with one hot encoding\n",
    "    # Pclass   - splits into (1 = Pclass_1, 2 = Pclass_2, 3 = Pclass_3)\n",
    "    # Embarked - splits into (C = Embarked_C, Q = Embarked_Q, S = Embarked_S)\n",
    "    # Deck - splits into decks with letters\n",
    "    df = pd.get_dummies(df, columns = ['Pclass', 'Embarked', 'Deck']) #, 'Title'])\n",
    "\n",
    "    # Drop columns with data deemed not relevant for learning\n",
    "    # Name     - Gender already has its' own column. Only thing that might be interesting here is the title\n",
    "    # Ticket   - Ticket does not really say much, price and class are already included which says the most\n",
    "    # Cabin    - Replaced by Deck\n",
    "    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Age', 'SibSp', 'Parch'], axis=1)\n",
    "    \n",
    "    # Deck_T is not present in the train set and must be inserted for uniformity\n",
    "    if prediction_data:\n",
    "        m = df.shape[0]\n",
    "        df['Deck_T'] = pd.Series(np.zeros(m, dtype=int), index=df.index)\n",
    "    \n",
    "    # Normalize the data\n",
    "    norm_vals = ['Fare', 'FamilySize']\n",
    "    df[norm_vals] = (df[norm_vals] - df[norm_vals].min())/(df[norm_vals].max() - df[norm_vals].min())\n",
    "    \n",
    "    # Look at interesting metrics to find information about the preprocessing/feature engineering\n",
    "    if print_info:\n",
    "        if not prediction_data:\n",
    "            print('--------------------------------------------------------------------------------------')\n",
    "            print('SURVIVAL RATE')\n",
    "            print('--------------------------------------------------------------------------------------')\n",
    "            print('Overall survival rate: ' + str(df['Survived'].mean()))\n",
    "            print()\n",
    "            print(survival_by_plcass)\n",
    "            print()\n",
    "            print(survival_by_embark)\n",
    "            print()\n",
    "            print(survival_by_deck)\n",
    "            print()\n",
    "            \"\"\"\n",
    "            print(survival_by_title)\n",
    "            print()\n",
    "            print(survival_by_title_simplified)\n",
    "            print()\n",
    "            \"\"\"\n",
    "            print(survival_by_familysize)\n",
    "            print()\n",
    "            print(survival_by_child)\n",
    "        \"\"\"\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('TITLES')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('All titels: ')\n",
    "        print(unique_titles)\n",
    "        print()\n",
    "        print('Simplied titels: ')\n",
    "        print(unique_titles_simplified)\n",
    "        \"\"\"\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('SUMS')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.sum())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('DATA INFO')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.info())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('MISSING VALUES')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.isnull().sum())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('CORRELATIONS')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.corr())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "To score the model it is important that the scoring is not performed on data that also has been used to train the model. Otherwise the evaluation will be overly optimistic but the performance on independant data will most likely perform much worse. The solution is to exclude a fraction of the data during training so that it can later be used for evaluation. This set is usually called the cross validation set.\n",
    "\n",
    "Validation does however don't stop after training. And every time data has been used to make a decision then new data has to be used to evaluate how well that decision will hold up on new data. A best practice is to further divide the data into train/test/cross validation.\n",
    "\n",
    "- Train set - Data used to train the model\n",
    "- Cross validation - Data used for finding deciding between different models and hyperparamteres\n",
    "- Test - Data used to make a final evaluation on how the model will perform on unseen data\n",
    "\n",
    "In this competition the test set is already set aside by default, so only a simply train/validation split is performed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into train/validation set\n",
    "def split_data(df):\n",
    "    df_train = df.sample(frac = 0.8, random_state = 42)\n",
    "    df_val = df.drop(df_train.index)  \n",
    "    \n",
    "    X_train = df_train.drop(['Survived'], axis=1).values\n",
    "    y_train = df_train['Survived'].values\n",
    "    \n",
    "    X_val = df_val.drop(['Survived'], axis=1).values\n",
    "    y_val = df_val['Survived'].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "SURVIVAL RATE\n",
      "--------------------------------------------------------------------------------------\n",
      "Overall survival rate: 0.3838383838383838\n",
      "\n",
      "Pclass\n",
      "1    0.629630\n",
      "2    0.472826\n",
      "3    0.242363\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Embarked\n",
      "C    0.553571\n",
      "Q    0.389610\n",
      "S    0.336957\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Deck\n",
      "A          0.466667\n",
      "B          0.744681\n",
      "C          0.593220\n",
      "D          0.757576\n",
      "E          0.757576\n",
      "F          0.583333\n",
      "G          0.500000\n",
      "T          0.000000\n",
      "Unknown    0.299854\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "FamilySize\n",
      "0     0.303538\n",
      "1     0.552795\n",
      "2     0.578431\n",
      "3     0.724138\n",
      "4     0.200000\n",
      "5     0.136364\n",
      "6     0.333333\n",
      "7     0.000000\n",
      "10    0.000000\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Child\n",
      "0    0.367397\n",
      "1    0.579710\n",
      "Name: Survived, dtype: float64\n",
      "--------------------------------------------------------------------------------------\n",
      "SUMS\n",
      "--------------------------------------------------------------------------------------\n",
      "Survived        342.000000\n",
      "Sex             314.000000\n",
      "Fare             56.006859\n",
      "FamilySize       80.600000\n",
      "Child            69.000000\n",
      "Pclass_1        216.000000\n",
      "Pclass_2        184.000000\n",
      "Pclass_3        491.000000\n",
      "Embarked_C      168.000000\n",
      "Embarked_Q       77.000000\n",
      "Embarked_S      644.000000\n",
      "Deck_A           15.000000\n",
      "Deck_B           47.000000\n",
      "Deck_C           59.000000\n",
      "Deck_D           33.000000\n",
      "Deck_E           33.000000\n",
      "Deck_F           12.000000\n",
      "Deck_G            4.000000\n",
      "Deck_T            1.000000\n",
      "Deck_Unknown    687.000000\n",
      "dtype: float64\n",
      "--------------------------------------------------------------------------------------\n",
      "DATA INFO\n",
      "--------------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 20 columns):\n",
      "Survived        891 non-null int64\n",
      "Sex             891 non-null int64\n",
      "Fare            891 non-null float64\n",
      "FamilySize      891 non-null float64\n",
      "Child           891 non-null int64\n",
      "Pclass_1        891 non-null uint8\n",
      "Pclass_2        891 non-null uint8\n",
      "Pclass_3        891 non-null uint8\n",
      "Embarked_C      891 non-null uint8\n",
      "Embarked_Q      891 non-null uint8\n",
      "Embarked_S      891 non-null uint8\n",
      "Deck_A          891 non-null uint8\n",
      "Deck_B          891 non-null uint8\n",
      "Deck_C          891 non-null uint8\n",
      "Deck_D          891 non-null uint8\n",
      "Deck_E          891 non-null uint8\n",
      "Deck_F          891 non-null uint8\n",
      "Deck_G          891 non-null uint8\n",
      "Deck_T          891 non-null uint8\n",
      "Deck_Unknown    891 non-null uint8\n",
      "dtypes: float64(2), int64(3), uint8(15)\n",
      "memory usage: 47.9 KB\n",
      "None\n",
      "--------------------------------------------------------------------------------------\n",
      "MISSING VALUES\n",
      "--------------------------------------------------------------------------------------\n",
      "Survived        0\n",
      "Sex             0\n",
      "Fare            0\n",
      "FamilySize      0\n",
      "Child           0\n",
      "Pclass_1        0\n",
      "Pclass_2        0\n",
      "Pclass_3        0\n",
      "Embarked_C      0\n",
      "Embarked_Q      0\n",
      "Embarked_S      0\n",
      "Deck_A          0\n",
      "Deck_B          0\n",
      "Deck_C          0\n",
      "Deck_D          0\n",
      "Deck_E          0\n",
      "Deck_F          0\n",
      "Deck_G          0\n",
      "Deck_T          0\n",
      "Deck_Unknown    0\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------------\n",
      "CORRELATIONS\n",
      "--------------------------------------------------------------------------------------\n",
      "              Survived       Sex      Fare  FamilySize     Child  Pclass_1  \\\n",
      "Survived      1.000000  0.543351  0.257307    0.016639  0.116691  0.285904   \n",
      "Sex           0.543351  1.000000  0.182333    0.200988  0.067534  0.098013   \n",
      "Fare          0.257307  0.182333  1.000000    0.217138 -0.003896  0.591711   \n",
      "FamilySize    0.016639  0.200988  0.217138    1.000000  0.425954 -0.046114   \n",
      "Child         0.116691  0.067534 -0.003896    0.425954  1.000000 -0.124702   \n",
      "Pclass_1      0.285904  0.098013  0.591711   -0.046114 -0.124702  1.000000   \n",
      "Pclass_2      0.093349  0.064746 -0.118557   -0.038594  0.028534 -0.288585   \n",
      "Pclass_3     -0.322308 -0.137143 -0.413333    0.071142  0.084221 -0.626738   \n",
      "Embarked_C    0.168240  0.082853  0.269335   -0.046215 -0.021578  0.296423   \n",
      "Embarked_Q    0.003650  0.074115 -0.117216   -0.058592 -0.029334 -0.155342   \n",
      "Embarked_S   -0.155660 -0.125722 -0.166603    0.079977  0.038722 -0.170379   \n",
      "Deck_A        0.022287 -0.078271  0.019549   -0.051767 -0.005275  0.231323   \n",
      "Deck_B        0.175095  0.109689  0.386297    0.004620 -0.049586  0.417160   \n",
      "Deck_C        0.114652  0.058649  0.364318    0.035347 -0.043381  0.470749   \n",
      "Deck_D        0.150716  0.079248  0.098878   -0.021566 -0.056820  0.291218   \n",
      "Deck_E        0.150716  0.054368  0.051749   -0.028937 -0.034586  0.235748   \n",
      "Deck_F        0.047930 -0.004667 -0.032495    0.006912  0.111859 -0.066095   \n",
      "Deck_G        0.016040  0.091031 -0.025180    0.035206  0.106163 -0.037988   \n",
      "Deck_T       -0.026456 -0.024728  0.002224   -0.018804 -0.009712  0.059256   \n",
      "Deck_Unknown -0.316912 -0.140391 -0.482075    0.009175  0.047949 -0.788773   \n",
      "\n",
      "              Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S  \\\n",
      "Survived      0.093349 -0.322308    0.168240    0.003650   -0.155660   \n",
      "Sex           0.064746 -0.137143    0.082853    0.074115   -0.125722   \n",
      "Fare         -0.118557 -0.413333    0.269335   -0.117216   -0.166603   \n",
      "FamilySize   -0.038594  0.071142   -0.046215   -0.058592    0.079977   \n",
      "Child         0.028534  0.084221   -0.021578   -0.029334    0.038722   \n",
      "Pclass_1     -0.288585 -0.626738    0.296423   -0.155342   -0.170379   \n",
      "Pclass_2      1.000000 -0.565210   -0.125416   -0.127301    0.192061   \n",
      "Pclass_3     -0.565210  1.000000   -0.153329    0.237449   -0.009511   \n",
      "Embarked_C   -0.125416 -0.153329    1.000000   -0.148258   -0.778359   \n",
      "Embarked_Q   -0.127301  0.237449   -0.148258    1.000000   -0.496624   \n",
      "Embarked_S    0.192061 -0.009511   -0.778359   -0.496624    1.000000   \n",
      "Deck_A       -0.066756 -0.144979    0.093040   -0.040246   -0.055383   \n",
      "Deck_B       -0.120386 -0.261450    0.168642   -0.072579   -0.123057   \n",
      "Deck_C       -0.135851 -0.295036    0.113952   -0.049776   -0.066995   \n",
      "Deck_D       -0.041325 -0.217282    0.102977   -0.060318   -0.051139   \n",
      "Deck_E       -0.041325 -0.169489   -0.003376   -0.039167    0.028520   \n",
      "Deck_F        0.132819 -0.051148   -0.056322   -0.001283    0.050608   \n",
      "Deck_G       -0.034258  0.060612   -0.032371   -0.020654    0.041589   \n",
      "Deck_T       -0.017100 -0.037138   -0.016158   -0.010310    0.020759   \n",
      "Deck_Unknown  0.172413  0.539291   -0.208528    0.129572    0.110087   \n",
      "\n",
      "                Deck_A    Deck_B    Deck_C    Deck_D    Deck_E    Deck_F  \\\n",
      "Survived      0.022287  0.175095  0.114652  0.150716  0.150716  0.047930   \n",
      "Sex          -0.078271  0.109689  0.058649  0.079248  0.054368 -0.004667   \n",
      "Fare          0.019549  0.386297  0.364318  0.098878  0.051749 -0.032495   \n",
      "FamilySize   -0.051767  0.004620  0.035347 -0.021566 -0.028937  0.006912   \n",
      "Child        -0.005275 -0.049586 -0.043381 -0.056820 -0.034586  0.111859   \n",
      "Pclass_1      0.231323  0.417160  0.470749  0.291218  0.235748 -0.066095   \n",
      "Pclass_2     -0.066756 -0.120386 -0.135851 -0.041325 -0.041325  0.132819   \n",
      "Pclass_3     -0.144979 -0.261450 -0.295036 -0.217282 -0.169489 -0.051148   \n",
      "Embarked_C    0.093040  0.168642  0.113952  0.102977 -0.003376 -0.056322   \n",
      "Embarked_Q   -0.040246 -0.072579 -0.049776 -0.060318 -0.039167 -0.001283   \n",
      "Embarked_S   -0.055383 -0.123057 -0.066995 -0.051139  0.028520  0.050608   \n",
      "Deck_A        1.000000 -0.030880 -0.034846 -0.025663 -0.025663 -0.015289   \n",
      "Deck_B       -0.030880  1.000000 -0.062841 -0.046280 -0.046280 -0.027572   \n",
      "Deck_C       -0.034846 -0.062841  1.000000 -0.052225 -0.052225 -0.031114   \n",
      "Deck_D       -0.025663 -0.046280 -0.052225  1.000000 -0.038462 -0.022914   \n",
      "Deck_E       -0.025663 -0.046280 -0.052225 -0.038462  1.000000 -0.022914   \n",
      "Deck_F       -0.015289 -0.027572 -0.031114 -0.022914 -0.022914  1.000000   \n",
      "Deck_G       -0.008787 -0.015847 -0.017883 -0.013170 -0.013170 -0.007846   \n",
      "Deck_T       -0.004386 -0.007910 -0.008926 -0.006574 -0.006574 -0.003917   \n",
      "Deck_Unknown -0.240136 -0.433053 -0.488683 -0.359896 -0.359896 -0.214417   \n",
      "\n",
      "                Deck_G    Deck_T  Deck_Unknown  \n",
      "Survived      0.016040 -0.026456     -0.316912  \n",
      "Sex           0.091031 -0.024728     -0.140391  \n",
      "Fare         -0.025180  0.002224     -0.482075  \n",
      "FamilySize    0.035206 -0.018804      0.009175  \n",
      "Child         0.106163 -0.009712      0.047949  \n",
      "Pclass_1     -0.037988  0.059256     -0.788773  \n",
      "Pclass_2     -0.034258 -0.017100      0.172413  \n",
      "Pclass_3      0.060612 -0.037138      0.539291  \n",
      "Embarked_C   -0.032371 -0.016158     -0.208528  \n",
      "Embarked_Q   -0.020654 -0.010310      0.129572  \n",
      "Embarked_S    0.041589  0.020759      0.110087  \n",
      "Deck_A       -0.008787 -0.004386     -0.240136  \n",
      "Deck_B       -0.015847 -0.007910     -0.433053  \n",
      "Deck_C       -0.017883 -0.008926     -0.488683  \n",
      "Deck_D       -0.013170 -0.006574     -0.359896  \n",
      "Deck_E       -0.013170 -0.006574     -0.359896  \n",
      "Deck_F       -0.007846 -0.003917     -0.214417  \n",
      "Deck_G        1.000000 -0.002251     -0.123234  \n",
      "Deck_T       -0.002251  1.000000     -0.061513  \n",
      "Deck_Unknown -0.123234 -0.061513      1.000000  \n",
      "--------------------------------------------------------------------------------------\n",
      "X_train shape: (713, 19)\n",
      "y_train shape: (713,)\n",
      "X_val shape: (178, 19)\n",
      "y_val shape: (178,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Child</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Deck_A</th>\n",
       "      <th>Deck_B</th>\n",
       "      <th>Deck_C</th>\n",
       "      <th>Deck_D</th>\n",
       "      <th>Deck_E</th>\n",
       "      <th>Deck_F</th>\n",
       "      <th>Deck_G</th>\n",
       "      <th>Deck_T</th>\n",
       "      <th>Deck_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex      Fare  FamilySize  Child  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "0         0    0  0.014151         0.1      0         0         0         1   \n",
       "1         1    1  0.139136         0.1      0         1         0         0   \n",
       "2         1    1  0.015469         0.0      0         0         0         1   \n",
       "3         1    1  0.103644         0.1      0         1         0         0   \n",
       "4         0    0  0.015713         0.0      0         0         0         1   \n",
       "\n",
       "   Embarked_C  Embarked_Q  Embarked_S  Deck_A  Deck_B  Deck_C  Deck_D  Deck_E  \\\n",
       "0           0           0           1       0       0       0       0       0   \n",
       "1           1           0           0       0       0       1       0       0   \n",
       "2           0           0           1       0       0       0       0       0   \n",
       "3           0           0           1       0       0       1       0       0   \n",
       "4           0           0           1       0       0       0       0       0   \n",
       "\n",
       "   Deck_F  Deck_G  Deck_T  Deck_Unknown  \n",
       "0       0       0       0             1  \n",
       "1       0       0       0             0  \n",
       "2       0       0       0             1  \n",
       "3       0       0       0             0  \n",
       "4       0       0       0             1  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed = preprocess_dataframe(df, False, True)\n",
    "\n",
    "X_train, y_train, X_val, y_val = split_data(df_processed)\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_val shape: \" + str(X_test.shape))\n",
    "print (\"y_val shape: \" + str(y_test.shape))\n",
    "\n",
    "df_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decision_tree_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    dt_clf = sk.tree.DecisionTreeClassifier(max_depth=20)\n",
    "    dt_clf.fit (X_train, y_train)\n",
    "    print(dt_clf.score (X_val, y_val))\n",
    "    \n",
    "    return dt_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8089887640449438\n"
     ]
    }
   ],
   "source": [
    "dt_clf = decision_tree_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_clf(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    rf_clf = ske.RandomForestClassifier(n_estimators=50)\n",
    "    rf_clf.fit (X_train, y_train)\n",
    "    print(rf_clf.score (X_test, y_test))\n",
    "    \n",
    "    return rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8202247191011236\n"
     ]
    }
   ],
   "source": [
    "rf_clf = random_forest_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    gb_clf = ske.GradientBoostingClassifier(n_estimators=50)\n",
    "    gb_clf.fit (X_train, y_train)\n",
    "    print(gb_clf.score (X_val, y_val))\n",
    "    \n",
    "    return gb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314606741573034\n"
     ]
    }
   ],
   "source": [
    "gb_clf = gradient_boosting_clf(X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit (X_train, y_train)\n",
    "    print(lr_clf.score (X_val, y_val))\n",
    "    \n",
    "    return lr_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314606741573034\n"
     ]
    }
   ],
   "source": [
    "lr_clf = logistic_regression_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def support_vector_machine_clf(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Builds and trains a support vector machine classifier\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- Training features,numpy matrix (m, 11)\n",
    "    y_train -- Training features,numpy matrix (m, )\n",
    "    X_test -- Test features,numpy matrix (m, 11)\n",
    "    y_test -- Test features,numpy matrix (m, )\n",
    "        \n",
    "    Returns:\n",
    "    svm_clf -- Classifier, sklearn SVC\n",
    "    \"\"\"\n",
    "    \n",
    "    svm_clf = sk.svm.SVC(probability=True)\n",
    "    svm_clf.fit (X_train, y_train)\n",
    "    print(svm_clf.score (X_val, y_val))\n",
    "    \n",
    "    return svm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8089887640449438\n"
     ]
    }
   ],
   "source": [
    "svm_clf = support_vector_machine_clf(X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_clf(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Builds and trains a gaussian naive bayes classifier\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- Training features,numpy matrix (m, 11)\n",
    "    y_train -- Training features,numpy matrix (m, )\n",
    "    X_test -- Test features,numpy matrix (m, 11)\n",
    "    y_test -- Test features,numpy matrix (m, )\n",
    "        \n",
    "    Returns:\n",
    "    svm_clf -- Classifier, sklearn GaussianNB\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf.fit (X_train, y_train)\n",
    "    print(nb_clf.score (X_val, y_val))\n",
    "    \n",
    "    return nb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7191011235955056\n"
     ]
    }
   ],
   "source": [
    "nb_clf = naive_bayes_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_neighbors_clf(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Builds and trains a k-nearest neighbors classifier\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- Training features,numpy matrix (m, 11)\n",
    "    y_train -- Training features,numpy matrix (m, )\n",
    "    X_test -- Test features,numpy matrix (m, 11)\n",
    "    y_test -- Test features,numpy matrix (m, )\n",
    "        \n",
    "    Returns:\n",
    "    svm_clf -- Classifier, sklearn KNeighborsClassifier\n",
    "    \"\"\"\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=6)\n",
    "    knn_clf.fit (X_train, y_train)\n",
    "    print(knn_clf.score (X_val, y_val))\n",
    "    \n",
    "    return knn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8595505617977528\n"
     ]
    }
   ],
   "source": [
    "knn_clf = k_neighbors_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensamble_voting_clf(clfs, X_train, y_train, X_val, y_val, cv=20, score_individually=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Builds and trains an ensamble of classifiers which them vote together\n",
    "\n",
    "    Arguments:\n",
    "    clfs -- Classifiers with labels, List Tuple(String, clf)\n",
    "    \n",
    "    Returns:\n",
    "    e_clf -- Classifier, sklearn VotingClassifier\n",
    "    \"\"\"\n",
    "    \n",
    "    e_clf = ske.VotingClassifier(estimators=clfs, voting='hard') # Hard voting where majority rules\n",
    "    e_clf.fit (X_train, y_train)\n",
    "    \n",
    "    if score_individually:\n",
    "        for label, clf in clfs:\n",
    "            scores = cross_val_score(clf, X_val, y_val, cv=cv, scoring='accuracy')\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))    \n",
    "    \n",
    "    scores = cross_val_score(e_clf, X_val, y_test, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Voting Ensamble')) \n",
    "    \n",
    "    return e_clf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.11) [Decision Tree]\n",
      "Accuracy: 0.83 (+/- 0.10) [Random Forest]\n",
      "Accuracy: 0.80 (+/- 0.10) [Gradiant Boosting]\n",
      "Accuracy: 0.83 (+/- 0.12) [Logistic Regression]\n",
      "Accuracy: 0.81 (+/- 0.10) [SVM]\n",
      "Accuracy: 0.55 (+/- 0.17) [Naive Bayes]\n",
      "Accuracy: 0.81 (+/- 0.11) [K-Nearest]\n",
      "Accuracy: 0.86 (+/- 0.09) [Voting Ensamble]\n"
     ]
    }
   ],
   "source": [
    "clfs = ([('Decision Tree', dt_clf), ('Random Forest', rf_clf), ('Gradiant Boosting', gb_clf), \n",
    "         ('Logistic Regression', lr_clf), ('SVM', svm_clf), ('Naive Bayes', nb_clf), ('K-Nearest', knn_clf)])\n",
    "\n",
    "e_clf = ensamble_voting_clf(clfs, X_train, y_train, X_test, y_test, score_individually=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(df, clf, export_path):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Makes predictions X -> y and exports to csv\n",
    "\n",
    "    Arguments:\n",
    "    df -- Data to predict from, pandas DataFrame\n",
    "    clf -- classifier, Classifier, sklearn classifier object\n",
    "    export_path -- Path and name of file, String\n",
    "        \n",
    "    Returns:\n",
    "    df_pred -- prediction, pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract Ids\n",
    "    y1 = df['PassengerId'].values\n",
    "    \n",
    "    # Make predictions\n",
    "    df_process = preprocess_dataframe(df, prediction_data=True, print_info=False) \n",
    "    X = df_process.values\n",
    "    y2 = clf.predict(X)\n",
    "    \n",
    "    # Combine ids and predictions\n",
    "    y = np.column_stack((y1, y2))\n",
    "    \n",
    "    # Restore pandas df\n",
    "    df_pred = pd.DataFrame(y)\n",
    "    df_pred.columns = [\"PassengerId\", \"Survived\"]\n",
    "    \n",
    "    # Export\n",
    "    df_pred.to_csv(export_path, sep=',', index=False)\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "df_pred = predict(df_pred, svm_clf, 'C:/GitHub/kaggle/titanic/predictions/predictions_svm.csv')\n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=1):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_search_hyperparameters(clf, hyper_param, X_train, y_train, X_test, y_test, print_result=False):\n",
    "    \n",
    "    # run randomized search\n",
    "    n_iter_search = 10\n",
    "    rnd_clf = RandomizedSearchCV(clf, param_distributions=hyper_param, n_iter=n_iter_search)\n",
    "\n",
    "    start = time()\n",
    "    rnd_clf.fit(X_train, y_train)\n",
    "    \n",
    "    if print_result:\n",
    "        print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "              \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "        report(rnd_clf.cv_results_)\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        scores = cross_val_score(clf, X_test, y_test, cv=20, scoring='accuracy')\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "        print()\n",
    "        print(classification_report(y_true, y_pred))\n",
    "       \n",
    "    return rnd_clf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_hyperparameters(clf, hyper_param, X_train, y_train, X_test, y_test, print_result=False):\n",
    "    \n",
    "    # run grid search\n",
    "    grid_clf = GridSearchCV(clf, param_grid=hyper_param)\n",
    "    start = time()\n",
    "    grid_clf.fit(X_train, y_train)\n",
    "    \n",
    "    if print_result:\n",
    "        report(grid_clf.cv_results_)\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        scores = cross_val_score(clf, X_test, y_test, cv=20, scoring='accuracy')\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "        print()\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    return grid_clf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "hyper_param = {\"max_depth\": [3, None],\n",
    "               \"max_features\": stats.randint(3, X_train.shape[1]),\n",
    "               \"min_samples_split\": stats.randint(2, 11),\n",
    "               \"min_samples_leaf\": stats.randint(1, 11),\n",
    "               \"bootstrap\": [True, False],\n",
    "               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rf_rnd_clf = random_search_hyperparameters(rf_clf, hyper_param, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.823 (std: 0.009)\n",
      "Parameters: {'min_samples_split': 10, 'max_depth': 3, 'criterion': 'gini', 'min_samples_leaf': 1, 'bootstrap': True, 'max_features': 19}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.823 (std: 0.013)\n",
      "Parameters: {'min_samples_split': 3, 'max_depth': None, 'criterion': 'entropy', 'min_samples_leaf': 3, 'bootstrap': True, 'max_features': 19}\n",
      "\n",
      "Accuracy: 0.83 (+/- 0.11)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.86      0.86       113\n",
      "          1       0.76      0.77      0.76        65\n",
      "\n",
      "avg / total       0.83      0.83      0.83       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "hyper_param = {\"max_depth\": [3, None],\n",
    "               \"max_features\": [3, 10, X_train.shape[1]],\n",
    "               \"min_samples_split\": [2, 3, 10],\n",
    "               \"min_samples_leaf\": [1, 3, 10],\n",
    "               \"bootstrap\": [True, False],\n",
    "               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rf_grid_clf = grid_search_hyperparameters(rf_clf, hyper_param, X_train, y_train, X_test, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "hyper_param = {'C': stats.randint(10, 10000),\n",
    "               'gamma' : stats.uniform(0.001, 1),\n",
    "               'kernel': ['rbf']}\n",
    "\n",
    "svm_rnd_clf = random_search_hyperparameters(svm_clf, hyper_param, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.813 (std: 0.009)\n",
      "Parameters: {'gamma': 0.1, 'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Accuracy: 0.81 (+/- 0.10)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.87      0.85       113\n",
      "          1       0.75      0.71      0.73        65\n",
      "\n",
      "avg / total       0.81      0.81      0.81       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "hyper_param = {'C': [10, 100, 1000, 10000], \n",
    "               'gamma' : [0.001, 0.01, 0.1, 1], \n",
    "               'kernel': ['rbf']}\n",
    "\n",
    "svm_grid_clf = grid_search_hyperparameters(svm_clf, hyper_param, X_train, y_train, X_test, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "hyper_param = {'penalty': ['l1','l2'], \n",
    "               'C': stats.uniform(0.001,1000)}\n",
    "\n",
    "lr_rnd_clf = random_search_hyperparameters(lr_clf, hyper_param, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.799 (std: 0.012)\n",
      "Parameters: {'C': 0.1, 'penalty': 'l2'}\n",
      "\n",
      "Accuracy: 0.84 (+/- 0.11)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.85      0.86       113\n",
      "          1       0.75      0.78      0.77        65\n",
      "\n",
      "avg / total       0.83      0.83      0.83       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "hyper_param = {'penalty': ['l1','l2'], \n",
    "               'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "lr_grid_clf = grid_search_hyperparameters(lr_clf, hyper_param, X_train, y_train, X_test, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.07) [Voting Ensamble]\n"
     ]
    }
   ],
   "source": [
    "clfs = ([('Grid Random Forest', rf_grid_clf), ('Grid SVM', svm_grid_clf), ('Grid Logistic Regression', lr_grid_clf)])\n",
    "\n",
    "e_grid_clf = ensamble_voting_clf(clfs, X_train, y_train, X_test, y_test, score_individually=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.02) [Voting Ensamble]\n"
     ]
    }
   ],
   "source": [
    "rf_hyper_param = {\"max_depth\": [3, None],\n",
    "                  \"max_features\": [3, 10, X_train.shape[1]],\n",
    "                  \"min_samples_split\": [2, 3, 10],\n",
    "                  \"min_samples_leaf\": [1, 3, 10],\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "svm_hyper_param = {'C': [10, 100, 1000, 10000], \n",
    "                   'gamma' : [0.001, 0.01, 0.1, 1], \n",
    "                   'kernel': ['rbf']}\n",
    "\n",
    "rf_grid_clf_1 = grid_search_hyperparameters(rf_clf, rf_hyper_param, X_train, y_train, X_test, y_test)\n",
    "rf_grid_clf_2 = grid_search_hyperparameters(rf_clf, rf_hyper_param, X_train, y_train, X_test, y_test)\n",
    "rf_grid_clf_3 = grid_search_hyperparameters(rf_clf, rf_hyper_param, X_train, y_train, X_test, y_test)\n",
    "\n",
    "svm_grid_clf_1 = grid_search_hyperparameters(svm_clf, svm_hyper_param, X_train, y_train, X_test, y_test)\n",
    "svm_grid_clf_2 = grid_search_hyperparameters(svm_clf, svm_hyper_param, X_train, y_train, X_test, y_test)\n",
    "svm_grid_clf_3 = grid_search_hyperparameters(svm_clf, svm_hyper_param, X_train, y_train, X_test, y_test)\n",
    "\n",
    "clfs = ([('RF 1', rf_grid_clf_1), \n",
    "         ('RF 2', rf_grid_clf_2), \n",
    "         ('RF 3', rf_grid_clf_3), \n",
    "         ('SVM 1', svm_grid_clf_1), \n",
    "         ('SVM 2', svm_grid_clf_2), \n",
    "         ('SVM 3', svm_grid_clf_3)])\n",
    "\n",
    "e_grid_clf = ensamble_voting_clf(clfs, X_train, y_train, X_test, y_test, score_individually=False, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "df_pred = predict(df_pred, rf_grid_clf, 'C:/GitHub/kaggle/titanic/predictions/predictions_tuned_random_forest.csv')\n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(paramters):\n",
    "    \n",
    "    num_features = parameters['num_features']\n",
    "    X = tf.placeholder(tf.float32, [None, num_features], name='X')\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='y')\n",
    "\n",
    "    layers_dim = paramters['layers_dim']\n",
    "    \n",
    "    fc = tf.contrib.layers.stack(X, tf.contrib.layers.fully_connected, layers_dim)\n",
    "    Z = tf.contrib.layers.fully_connected(fc, 1, activation_fn=None, scope='Z')\n",
    "    \n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z, labels=y, name='Loss')\n",
    "    cost = tf.reduce_mean(loss, name='Cost')\n",
    "    \n",
    "    learning_rate = parameters['learning_rate']\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    prediction = tf.round(tf.sigmoid(Z))\n",
    "    correct_prediction = tf.equal(prediction, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    model = {'X': X, 'y': y, 'Z': Z, 'cost': cost,\n",
    "             'train_op': train_op, 'prediction': prediction, 'accuracy': accuracy}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X_train, Y_train, mini_batch_size = 32, seed = 0):\n",
    " \n",
    "    # Shuffle with identical seed to get same shuffle in both X and Y\n",
    "    np.random.seed(seed)\n",
    "    X_train = np.random.permutation(X_train)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    Y_train = np.random.permutation(Y_train)\n",
    "    \n",
    "    m = X_train.shape[0]\n",
    "    num_batches = int(m / mini_batch_size)\n",
    "    \n",
    "    # Split data into smaller batches for ready for stochastic gradient descent\n",
    "    minibatches_X = np.array_split(X_train, num_batches)\n",
    "    minibatches_Y = np.array_split(Y_train, num_batches)\n",
    "    \n",
    "    minibatches = zip(minibatches_X, minibatches_Y)\n",
    "    \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(parameters, model):\n",
    "    \n",
    "    num_epochs = parameters['num_epochs']\n",
    "    minibatch_size = parameters['minibatch_size']\n",
    "    X_train = parameters['X_train']\n",
    "    y_train = parameters['y_train']\n",
    "    \n",
    "    \n",
    "    train_size = X_train.shape[0]\n",
    "    saver = tf.train.Saver()\n",
    "    epoch_list = []\n",
    "    cost_list = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(train_size / minibatch_size)\n",
    "            minibatches = random_mini_batches(X_train, y_train, minibatch_size)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                (minibatch_X, minibatch_y) = minibatch\n",
    "                minibatch_y = np.reshape(minibatch_y, (minibatch_X.shape[0], 1))\n",
    "                feed_dict = {model['X'] : minibatch_X, model['y'] : minibatch_y}\n",
    "\n",
    "                _model ,minibatch_cost = sess.run([model['train_op'], model['cost']], feed_dict= feed_dict)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "            if parameters['print'] and (epoch % parameters['print_freq'] == 0):\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            \n",
    "            if parameters['save_cost'] and (epoch % parameters['save_cost_freq'] == 0):\n",
    "                epoch_list.append(epoch)\n",
    "                cost_list.append(epoch_cost)\n",
    "                \n",
    "        saver.save(sess, parameters['save_path'])\n",
    "        \n",
    "    return {'epoch_list': epoch_list, 'cost_list' : cost_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "\n",
    "# set model parameters\n",
    "parameters['X_train'] = X_train\n",
    "parameters['y_train'] = y_train\n",
    "parameters['X_test'] = X_test\n",
    "parameters['y_test'] = y_test\n",
    "parameters['X_pred'] = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "parameters['PassangerId'] = pd.DataFrame(parameters['X_pred']['PassengerId'], columns=['PassengerId'])\n",
    "parameters['layers_dim'] = [14]\n",
    "parameters['num_features'] = X_train.shape[1]\n",
    "parameters['layers_dim'] = [14]\n",
    "parameters['learning_rate'] = 0.01\n",
    "\n",
    "# set train parameters (hyper parameter)\n",
    "parameters['num_epochs'] = 3000\n",
    "parameters['minibatch_size'] = 20\n",
    "\n",
    "# set option parameters\n",
    "parameters['model_name'] = 'nn_clf'\n",
    "parameters['save_path'] = 'C:/GitHub/kaggle/titanic/models/' + parameters['model_name']\n",
    "parameters['print'] = True\n",
    "parameters['print_freq'] = 500\n",
    "parameters['save_cost'] = True\n",
    "parameters['save_cost_freq'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.594308\n",
      "Cost after epoch 500: 0.347162\n",
      "Cost after epoch 1000: 0.338047\n",
      "Cost after epoch 1500: 0.330413\n",
      "Cost after epoch 2000: 0.327844\n",
      "Cost after epoch 2500: 0.325424\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = create_model(parameters)\n",
    "    plot_data = train_model(parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253cb29d898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "print\n",
    "if parameters['save_cost']:\n",
    "    plt.plot(plot_data['epoch_list'], plot_data['cost_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(parameters, model):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, parameters['save_path'])\n",
    "        print (\"Train Accuracy:\", model['accuracy'].eval({model['X']: X_train, \n",
    "                                                          model['y']: np.reshape(y_train, (X_train.shape[0], 1)) }))\n",
    "        print (\"Valid Accuracy:\", model['accuracy'].eval({model['X']: X_test, \n",
    "                                                          model['y']: np.reshape(y_test, (X_test.shape[0], 1)) }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/GitHub/kaggle/titanic/models/nn_clf\n",
      "Train Accuracy: 0.8695652\n",
      "Valid Accuracy: 0.83707863\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = create_model(parameters)\n",
    "    evaluate(parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_nn(parameters, model):\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    X_pred = parameters['X_pred']\n",
    "    X_pred = preprocess_dataframe(X_pred, prediction_data=True, print_info=False)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, parameters['save_path'])\n",
    "        prediction = model['prediction'].eval({model['X']: X_pred})\n",
    "        return prediction.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/GitHub/kaggle/titanic/models/nn_clf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         1\n",
       "3          895         1\n",
       "4          896         0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = parameters['PassangerId']\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    model = create_model(parameters)\n",
    "    y_pred = predict_nn(parameters, model)\n",
    "    df_pred['Survived'] = y_pred\n",
    "    df_pred.to_csv('C:/GitHub/kaggle/titanic/predictions/predictions_neural_network.csv', sep=',', index=False)\n",
    "\n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

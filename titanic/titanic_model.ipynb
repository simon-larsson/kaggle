{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Passanger Survival Prediction\n",
    "\n",
    "This notebook contains my efforts and thoughts for the Titanic: Machine Learning from Disaster [Kaggle](https://www.kaggle.com/c/titanic) competition. The aim is to explore different machine learning concepts rather than to find the optimal score. It is also a practice in making a well documented and easy to follow notebook.\n",
    "\n",
    "### Covered topics\n",
    "- Data preprocessing using Pandas\n",
    "- Feature engineering\n",
    "- Conventional ML models using scikit-learn\n",
    "- Ensambles\n",
    "- Hyperparameter tuning\n",
    "- Neural network using Tensorflow\n",
    "\n",
    "### Things for another day\n",
    "- Visualizations\n",
    "- Regularization\n",
    "- Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import math\n",
    "from time import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removes a warning in sklearn that will be fixed during an update mid 2018\n",
    "import warnings\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "    le = sk.preprocessing.LabelEncoder()\n",
    "    le.fit([1, 2, 2, 6])\n",
    "    le.transform([1, 1, 2, 6])\n",
    "    le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "We will use widely used [Pandas](https://pandas.pydata.org/) to preprocess our data. We start with reading the data from .csv with read_csv(). When we have our dataframe in memory we can use head to take a first look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/GitHub/kaggle/titanic/data/train.csv', sep=',', header=0)\n",
    "print('Data size: ' + str(df.shape))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the data\n",
    "Now we can inspect our features. We have 11 feauters, which we will call X, and 1 output (Survived), which we will call y. We can also see that we have 891 training examples. Now let's go over our features and try to reason over them. The most important thing is to find data that we think might affect the chance of survival for a passenger.\n",
    "\n",
    "#### PassangerId\n",
    "This is just used for indentification when scoring the predictions. It does not contain any ground truth about passengers and their survival and should therefore be removed before training.\n",
    "\n",
    "#### Survived\n",
    "This is what we want to predict, our output y. It should be split from the features and kept separate.\n",
    "\n",
    "#### Pclass\n",
    "The fare class of the passenger. Pclass = 1 is first class, Pclass = 2 is second class and Pclass = 3 is third class. This feature will likely affect survival. Things such people density and the distribution of [life boats](https://en.wikipedia.org/wiki/Lifeboats_of_the_RMS_Titanic) will likely vary amoung classes and play a part in survival.\n",
    "\n",
    "#### Name\n",
    "There are two things with the name that might be interesting in terms of surivival. The name might give us the gender of the passenger. We can however see that the gender is already provided in another feature, so there is no point in extracting it here. The second, and more interesting thing, is the title. The title can tell us the marital and sociatal status of a passenger.\n",
    "\n",
    "#### Sex\n",
    "[\"Women and children first\"](https://en.wikipedia.org/wiki/Women_and_children_first) is famously associated with the Titanic. Therefore it is safe to assume that sex will play a large factor in survival.\n",
    "\n",
    "#### Age\n",
    "The same motivation as with sex can be applied as to why age would be important for survival.\n",
    "\n",
    "#### SibSp\n",
    "Number of siblings/spouses onboard. Probably will be a factor for survival since family members can help eachother.\n",
    "\n",
    "#### Parch\n",
    "Number of parents/children onboard. Very similar to the feature above.\n",
    "\n",
    "#### Ticket\n",
    "The tickets numbers seem to have some useful information in them. However, there seem to be little structure in the data. I decided to skip it to minimize initial effort. Might revise it later.\n",
    "\n",
    "#### Fare\n",
    "The price of the ticket. Will likely have similar factors as Pclass.\n",
    "\n",
    "#### Embarked\n",
    "Port of embarkation. Where C = Cherbourg, Q = Queenstown and S = Southampton. This should, at first glance, not play a big role in survival. However, it could possibly be a indirect factor and should not be dismissed without investigation.\n",
    "\n",
    "### Data size\n",
    "891 samples of data is on the low end for machine learning. It will mean that the model might be subseptible to overfitting. Overfitting is when to model specializes too much on the given dataset instead of learning a ground truth which that can help with predicting previously unseen data. It also means that it might be reasonable to chose simpler, conventional, machine learning models instead of the more data hungry neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils\n",
    "Below are just some util functions that will later be used for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checks and returns if a string contains one of a list of given substrings.\n",
    "def substrings_in_string(whole, subs): \n",
    "    \n",
    "    for x in subs:\n",
    "        if x in str(whole): \n",
    "            return x\n",
    "        \n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reducing data by combining decks with similar survival rates\n",
    "def reduce_decks(x):\n",
    "    \n",
    "    reduced_decks = {\n",
    "        \"A\": \"AG\",\n",
    "        \"B\": \"BDE\",\n",
    "        \"C\": \"CF\",\n",
    "        \"D\": \"BDE\",\n",
    "        \"E\": \"BDE\",\n",
    "        \"F\": \"CF\",\n",
    "        \"G\": \"AG\",\n",
    "        \"T\": \"UT\",\n",
    "        \"Unknown\": \"UT\"}\n",
    "    \n",
    "    return reduced_decks[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract title from a name\n",
    "def extract_title(full_name):\n",
    "    \n",
    "    full_name  = str(full_name)\n",
    "    \n",
    "    x = full_name.split(\", \")\n",
    "    x = x[1]\n",
    "    x = x.split('.')\n",
    "    title = x[0]\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplify titles into fewer more significant titles\n",
    "def simplify_titles(x):\n",
    "    \n",
    "    titles = {\n",
    "    \"Capt\": \"Officer\",\n",
    "    \"Col\": \"Officer\",\n",
    "    \"Major\": \"Officer\",\n",
    "    \"Jonkheer\": \"Aristocrat\",\n",
    "    \"Don\": \"Aristocrat\",\n",
    "    \"Dona\": \"Aristocrat\",\n",
    "    \"Sir\" : \"Aristocrat\",\n",
    "    \"Dr\": \"Officer\",\n",
    "    \"Rev\": \"Officer\",\n",
    "    \"the Countess\":\"Aristocrat\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Mrs\",\n",
    "    \"Mr\" : \"Mr\",\n",
    "    \"Mrs\" : \"Mrs\",\n",
    "    \"Miss\" : \"Miss\",\n",
    "    \"Master\" : \"Master\",\n",
    "    \"Lady\" : \"Aristocrat\"\n",
    "    }\n",
    "    \n",
    "    return titles[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and feature engineering\n",
    "Data preprocessing and feature engineering go hand in hand and have the aim find the best possible data for a model. They are together probably the most important part for the success of a model, especially when the data size is small. It is not something that is performed once and then forgotten. Data should be revised and optimized in iterations together with models.\n",
    "\n",
    "### Data preprocessing\n",
    "Data preprocessing is when data is adjusted to be recieved by a model. The following processing is performed:\n",
    "- Removing noise - Data not relevant for the prediction will make the model more noisy and should be removed.\n",
    "- Missing values - Values that are missing are filled with mean of most common occuring value.\n",
    "- Normalization - If data on different scales will serve as initial weights for the model. Therefor it is good to normalize the data and let the model find the weights be itself. A good normalization is to have all data on a magnititude of 0.0 - 1.0.\n",
    "- Data discretation - Data that represents different categories (such as Pclass) can be represented as separate discreet features instead of quanties.\n",
    "\n",
    "### Feature engineering\n",
    "Feature engineering is when domain knowledge is applied to find new features that will be more useful for the model. The following techniques were used.\n",
    "- Indicator Variables - Quantities can be divided into meaningful brackets. Example: Age can be divided into categories to find children since we know that they are more likely to live.\n",
    "- Interaction Features - Features can sometimes be combined into something meaningful. Example: SibSp and Parch can be combined to find the family size.\n",
    "- Grouping sparse data - Categorical data that has low occurence can be grouped into larger meaningful categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, prediction_data=False, print_info=False):\n",
    "    # Age has missing values which is replaced with average\n",
    "    # Might also consider dividing age into classes of age brackets\n",
    "    df['Age'].fillna((df['Age'].mean()), inplace = True)\n",
    "    df['Fare'].fillna((df['Fare'].mean()), inplace = True)\n",
    "    \n",
    "    # Encode sex into binary (0 = male, 1 = female)\n",
    "    df['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n",
    "    \n",
    "    # Use the prefix in cabin to find the deck of the cabin\n",
    "    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n",
    "    df['Deck' ]= df['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "    df['Deck' ]= df['Deck'].map(lambda x: reduce_decks(x))\n",
    "    \n",
    "    # Calculate family size by combining SibSp and Parch\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "    \n",
    "    # Divide age into useful brackets\n",
    "    df['Child'] = df['Age'].apply(lambda x: 1 if x <= 12 else 0)\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract and process titles\n",
    "    df['Title']= df['Name'].map(lambda x: extract_title(x))\n",
    "    if not prediction_data:\n",
    "        unique_titles = df['Title'].unique()\n",
    "        survival_by_title = df.groupby('Title').mean()['Survived']\n",
    "    \n",
    "    df['Title']= df['Title'].map(lambda x: simplify_titles(x))\n",
    "    if not prediction_data:\n",
    "        unique_titles_simplified = df['Title'].unique()\n",
    "        survival_by_title_simplified = df.groupby('Title').mean()['Survived']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gather info on the significance of these classes for survival\n",
    "    # Class was, as expected, significance for surival with the rates (1st - 63%, 2nd - 47%, 3rd - 24%)\n",
    "    # Embarked was suprisingly significant, C - Cherbourg had 55% surivial rate when the mean was just 38%\n",
    "    if not prediction_data:\n",
    "        survival_by_plcass = df.groupby('Pclass').mean()['Survived']\n",
    "        survival_by_deck = df.groupby('Deck').mean()['Survived']\n",
    "        survival_by_embark = df.groupby('Embarked').mean()['Survived']\n",
    "        survival_by_family = df.groupby('FamilySize').mean()['Survived']\n",
    "        survival_by_child = df.groupby('Child').mean()['Survived']\n",
    "    \n",
    "    # Split classes with one hot encoding\n",
    "    # Pclass   - splits into (1 = Pclass_1, 2 = Pclass_2, 3 = Pclass_3)\n",
    "    # Embarked - splits into (C = Embarked_C, Q = Embarked_Q, S = Embarked_S)\n",
    "    # Deck - splits into decks with letters\n",
    "    df = pd.get_dummies(df, columns = ['Pclass', 'Embarked', 'Deck']) #, 'Title'])\n",
    "\n",
    "    # Drop columns with data deemed not relevant for learning\n",
    "    # Name     - Gender already has its' own column. Only thing that might be interesting here is the title\n",
    "    # Ticket   - Ticket does not really say much, price and class are already included which says the most\n",
    "    # Cabin    - Replaced by Deck\n",
    "    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Age', 'SibSp', 'Parch'], axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Deck_T is not present in the train set and must be inserted for uniformity\n",
    "    if prediction_data:\n",
    "        m = df.shape[0]\n",
    "        df['Deck_T'] = pd.Series(np.zeros(m, dtype=int), index=df.index)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the data\n",
    "    norm_vals = ['Fare', 'FamilySize']\n",
    "    df[norm_vals] = (df[norm_vals] - df[norm_vals].min())/(df[norm_vals].max() - df[norm_vals].min())\n",
    "    \n",
    "    # Look at interesting metrics to find information about the preprocessing/feature engineering\n",
    "    if print_info:\n",
    "        if not prediction_data:\n",
    "            print('--------------------------------------------------------------------------------------')\n",
    "            print('SURVIVAL RATE')\n",
    "            print('--------------------------------------------------------------------------------------')\n",
    "            print('Overall survival rate: ' + str(df['Survived'].mean()))\n",
    "            print()\n",
    "            print(survival_by_plcass.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_embark.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_deck.sort_values(ascending=False))\n",
    "            print()\n",
    "            \"\"\"\n",
    "            print(survival_by_title.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_title_simplified.sort_values(ascending=False))\n",
    "            print()\n",
    "            \"\"\"\n",
    "            print(survival_by_family.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_child.sort_values(ascending=False))\n",
    "        \"\"\"\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('TITLES')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('All titels: ')\n",
    "        print(unique_titles)\n",
    "        print()\n",
    "        print('Simplied titels: ')\n",
    "        print(unique_titles_simplified)\n",
    "        \"\"\"\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('SUMS')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.sum())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('DATA INFO')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.info())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('MISSING VALUES')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.isnull().sum())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('CORRELATIONS')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.corr())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "To score the model it is important that the scoring is not performed on data that also has been used to train the model. Otherwise the evaluation will be overly optimistic but the performance on independant data will most likely perform much worse. The solution is to exclude a fraction of the data during training so that it can later be used for evaluation. This set is usually called the cross validation set.\n",
    "\n",
    "Validation does however don't stop after training. And every time data has been used to make a decision then new data has to be used to evaluate how well that decision will hold up on new data. A best practice is to further divide the data into train/test/cross validation.\n",
    "\n",
    "- Train set - Data used to train the model\n",
    "- Cross validation - Data used for finding deciding between different models and hyperparamteres\n",
    "- Test - Data used to make a final evaluation on how the model will perform on unseen data\n",
    "\n",
    "In this competition the test set is already set aside by default, so only a simply train/validation split is performed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into train/validation set\n",
    "def split_data(df):\n",
    "    df_train = df.sample(frac = 0.8, random_state = 42)\n",
    "    df_val = df.drop(df_train.index)  \n",
    "    \n",
    "    X_train = df_train.drop(['Survived'], axis=1).values\n",
    "    y_train = df_train['Survived'].values\n",
    "    \n",
    "    X_val = df_val.drop(['Survived'], axis=1).values\n",
    "    y_val = df_val['Survived'].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "SURVIVAL RATE\n",
      "--------------------------------------------------------------------------------------\n",
      "Overall survival rate: 0.3838383838383838\n",
      "\n",
      "Pclass\n",
      "1    0.629630\n",
      "2    0.472826\n",
      "3    0.242363\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Embarked\n",
      "C    0.553571\n",
      "Q    0.389610\n",
      "S    0.336957\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Deck\n",
      "BDE    0.752212\n",
      "CF     0.591549\n",
      "AG     0.473684\n",
      "UT     0.299419\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "FamilySize\n",
      "3     0.724138\n",
      "2     0.578431\n",
      "1     0.552795\n",
      "6     0.333333\n",
      "0     0.303538\n",
      "4     0.200000\n",
      "5     0.136364\n",
      "10    0.000000\n",
      "7     0.000000\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Child\n",
      "1    0.579710\n",
      "0    0.367397\n",
      "Name: Survived, dtype: float64\n",
      "--------------------------------------------------------------------------------------\n",
      "SUMS\n",
      "--------------------------------------------------------------------------------------\n",
      "Survived      342.000000\n",
      "Sex           314.000000\n",
      "Fare           56.006859\n",
      "FamilySize     80.600000\n",
      "Child          69.000000\n",
      "Pclass_1      216.000000\n",
      "Pclass_2      184.000000\n",
      "Pclass_3      491.000000\n",
      "Embarked_C    168.000000\n",
      "Embarked_Q     77.000000\n",
      "Embarked_S    644.000000\n",
      "Deck_AG        19.000000\n",
      "Deck_BDE      113.000000\n",
      "Deck_CF        71.000000\n",
      "Deck_UT       688.000000\n",
      "dtype: float64\n",
      "--------------------------------------------------------------------------------------\n",
      "DATA INFO\n",
      "--------------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      "Survived      891 non-null int64\n",
      "Sex           891 non-null int64\n",
      "Fare          891 non-null float64\n",
      "FamilySize    891 non-null float64\n",
      "Child         891 non-null int64\n",
      "Pclass_1      891 non-null uint8\n",
      "Pclass_2      891 non-null uint8\n",
      "Pclass_3      891 non-null uint8\n",
      "Embarked_C    891 non-null uint8\n",
      "Embarked_Q    891 non-null uint8\n",
      "Embarked_S    891 non-null uint8\n",
      "Deck_AG       891 non-null uint8\n",
      "Deck_BDE      891 non-null uint8\n",
      "Deck_CF       891 non-null uint8\n",
      "Deck_UT       891 non-null uint8\n",
      "dtypes: float64(2), int64(3), uint8(10)\n",
      "memory usage: 43.6 KB\n",
      "None\n",
      "--------------------------------------------------------------------------------------\n",
      "MISSING VALUES\n",
      "--------------------------------------------------------------------------------------\n",
      "Survived      0\n",
      "Sex           0\n",
      "Fare          0\n",
      "FamilySize    0\n",
      "Child         0\n",
      "Pclass_1      0\n",
      "Pclass_2      0\n",
      "Pclass_3      0\n",
      "Embarked_C    0\n",
      "Embarked_Q    0\n",
      "Embarked_S    0\n",
      "Deck_AG       0\n",
      "Deck_BDE      0\n",
      "Deck_CF       0\n",
      "Deck_UT       0\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------------\n",
      "CORRELATIONS\n",
      "--------------------------------------------------------------------------------------\n",
      "            Survived       Sex      Fare  FamilySize     Child  Pclass_1  \\\n",
      "Survived    1.000000  0.543351  0.257307    0.016639  0.116691  0.285904   \n",
      "Sex         0.543351  1.000000  0.182333    0.200988  0.067534  0.098013   \n",
      "Fare        0.257307  0.182333  1.000000    0.217138 -0.003896  0.591711   \n",
      "FamilySize  0.016639  0.200988  0.217138    1.000000  0.425954 -0.046114   \n",
      "Child       0.116691  0.067534 -0.003896    0.425954  1.000000 -0.124702   \n",
      "Pclass_1    0.285904  0.098013  0.591711   -0.046114 -0.124702  1.000000   \n",
      "Pclass_2    0.093349  0.064746 -0.118557   -0.038594  0.028534 -0.288585   \n",
      "Pclass_3   -0.322308 -0.137143 -0.413333    0.071142  0.084221 -0.626738   \n",
      "Embarked_C  0.168240  0.082853  0.269335   -0.046215 -0.021578  0.296423   \n",
      "Embarked_Q  0.003650  0.074115 -0.117216   -0.058592 -0.029334 -0.155342   \n",
      "Embarked_S -0.155660 -0.125722 -0.166603    0.079977  0.038722 -0.170379   \n",
      "Deck_AG     0.027271 -0.027579  0.005757   -0.029810  0.044431  0.188427   \n",
      "Deck_BDE    0.288680  0.149509  0.344967   -0.025558 -0.085182  0.579273   \n",
      "Deck_CF     0.125678  0.051867  0.320696    0.035399  0.007779  0.404123   \n",
      "Deck_UT    -0.319572 -0.142608 -0.482732    0.007690  0.047257 -0.785408   \n",
      "\n",
      "            Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S   Deck_AG  \\\n",
      "Survived    0.093349 -0.322308    0.168240    0.003650   -0.155660  0.027271   \n",
      "Sex         0.064746 -0.137143    0.082853    0.074115   -0.125722 -0.027579   \n",
      "Fare       -0.118557 -0.413333    0.269335   -0.117216   -0.166603  0.005757   \n",
      "FamilySize -0.038594  0.071142   -0.046215   -0.058592    0.079977 -0.029810   \n",
      "Child       0.028534  0.084221   -0.021578   -0.029334    0.038722  0.044431   \n",
      "Pclass_1   -0.288585 -0.626738    0.296423   -0.155342   -0.170379  0.188427   \n",
      "Pclass_2    1.000000 -0.565210   -0.125416   -0.127301    0.192061 -0.075304   \n",
      "Pclass_3   -0.565210  1.000000   -0.153329    0.237449   -0.009511 -0.101063   \n",
      "Embarked_C -0.125416 -0.153329    1.000000   -0.148258   -0.778359  0.067878   \n",
      "Embarked_Q -0.127301  0.237449   -0.148258    1.000000   -0.496624 -0.045400   \n",
      "Embarked_S  0.192061 -0.009511   -0.778359   -0.496624    1.000000 -0.030076   \n",
      "Deck_AG    -0.075304 -0.101063    0.067878   -0.045400   -0.030076  1.000000   \n",
      "Deck_BDE   -0.127770 -0.395118    0.169805   -0.105212   -0.095497 -0.056256   \n",
      "Deck_CF    -0.068209 -0.292682    0.080661   -0.046252   -0.039976 -0.043435   \n",
      "Deck_UT     0.171347  0.537260   -0.210178    0.128973    0.111935 -0.271747   \n",
      "\n",
      "            Deck_BDE   Deck_CF   Deck_UT  \n",
      "Survived    0.288680  0.125678 -0.319572  \n",
      "Sex         0.149509  0.051867 -0.142608  \n",
      "Fare        0.344967  0.320696 -0.482732  \n",
      "FamilySize -0.025558  0.035399  0.007690  \n",
      "Child      -0.085182  0.007779  0.047257  \n",
      "Pclass_1    0.579273  0.404123 -0.785408  \n",
      "Pclass_2   -0.127770 -0.068209  0.171347  \n",
      "Pclass_3   -0.395118 -0.292682  0.537260  \n",
      "Embarked_C  0.169805  0.080661 -0.210178  \n",
      "Embarked_Q -0.105212 -0.046252  0.128973  \n",
      "Embarked_S -0.095497 -0.039976  0.111935  \n",
      "Deck_AG    -0.056256 -0.043435 -0.271747  \n",
      "Deck_BDE    1.000000 -0.112143 -0.701610  \n",
      "Deck_CF    -0.112143  1.000000 -0.541712  \n",
      "Deck_UT    -0.701610 -0.541712  1.000000  \n",
      "--------------------------------------------------------------------------------------\n",
      "X_train shape: (713, 14)\n",
      "y_train shape: (713,)\n",
      "X_val shape: (178, 14)\n",
      "y_val shape: (178,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Child</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Deck_AG</th>\n",
       "      <th>Deck_BDE</th>\n",
       "      <th>Deck_CF</th>\n",
       "      <th>Deck_UT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex      Fare  FamilySize  Child  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "0         0    0  0.014151         0.1      0         0         0         1   \n",
       "1         1    1  0.139136         0.1      0         1         0         0   \n",
       "2         1    1  0.015469         0.0      0         0         0         1   \n",
       "3         1    1  0.103644         0.1      0         1         0         0   \n",
       "4         0    0  0.015713         0.0      0         0         0         1   \n",
       "\n",
       "   Embarked_C  Embarked_Q  Embarked_S  Deck_AG  Deck_BDE  Deck_CF  Deck_UT  \n",
       "0           0           0           1        0         0        0        1  \n",
       "1           1           0           0        0         0        1        0  \n",
       "2           0           0           1        0         0        0        1  \n",
       "3           0           0           1        0         0        1        0  \n",
       "4           0           0           1        0         0        0        1  "
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed = preprocess_dataframe(df, False, True)\n",
    "\n",
    "X_train, y_train, X_val, y_val = split_data(df_processed)\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_val shape: \" + str(X_val.shape))\n",
    "print (\"y_val shape: \" + str(y_val.shape))\n",
    "\n",
    "df_processed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Machine Learning Models\n",
    "Today the hype is all neural networks, but they are not strictly better and it is often better to start with conventional machine learning algorithms. They are faster to implement and can often give a better result. Neural networks are notouriously heavy on computation and usually require lots of data before they outperform conventional algorithms.\n",
    "\n",
    "In our case we have a very small training set, only 891 samples. So starting with conventional algorithms definately feels like the right choice. The strategy here will be to cast a wide net and try a wide variety of algorithms that can perform classification and then narrow it down to those who perform the best.\n",
    "\n",
    "To implement the models we use scikit-learn, which is more or less the defacto framework for conventional ML. All classifiers used here are not really top candidates, but since scikit-learn makes it so easy to implement, we will still use quite a few just to learn. The algorithms we will use are:\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- Gradient boosting\n",
    "- Logistic regression\n",
    "- Support vector machine\n",
    "- Naive Bayes\n",
    "- K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree \n",
    "[Decision Tree](http://scikit-learn.org/stable/modules/tree.html) is a simple algorithm. It builds a tree of nested if-then-else statements based on the input features in order to make a prediction. It is generally weak to overfitting when the ratio of data/features is low, which unfortunately is our case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decision_tree_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    dt_clf = sk.tree.DecisionTreeClassifier(max_depth=20)\n",
    "    dt_clf.fit (X_train, y_train)\n",
    "    print(dt_clf.score (X_val, y_val))\n",
    "    \n",
    "    return dt_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8089887640449438\n"
     ]
    }
   ],
   "source": [
    "dt_clf = decision_tree_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "[Random forest](http://scikit-learn.org/stable/modules/tree.html) is an ensamble algorithms. That means that it combines more than one instance of one or more algorithms in order to make a prediction. Random forest divides the data into smaller subsets and trains a decision tree on each set which is then averaged to make the final prediction. This helps counteract the overfitting that is seen in decision trees and is generally better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_clf(X_train, y_train, X_val, y_val):\n",
    "\n",
    "    rf_clf = ske.RandomForestClassifier(n_estimators=50)\n",
    "    rf_clf.fit (X_train, y_train)\n",
    "    print(rf_clf.score (X_val, y_val))\n",
    "    \n",
    "    return rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8202247191011236\n"
     ]
    }
   ],
   "source": [
    "rf_clf = random_forest_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting \n",
    "[Gradient Boosting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) is another example of ensamble algorithms. It is a bit more involved than random forest and is not easy to summerize. You can say that it iteratively adds weaker models together weights them based on accuracy in order to output as one strong model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    gb_clf = ske.GradientBoostingClassifier(n_estimators=50)\n",
    "    gb_clf.fit (X_train, y_train)\n",
    "    print(gb_clf.score (X_val, y_val))\n",
    "    \n",
    "    return gb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314606741573034\n"
     ]
    }
   ],
   "source": [
    "gb_clf = gradient_boosting_clf(X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "[Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is an algorithm where a linear boundary is created that divides the positive and negative predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit (X_train, y_train)\n",
    "    print(lr_clf.score (X_val, y_val))\n",
    "    \n",
    "    return lr_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314606741573034\n"
     ]
    }
   ],
   "source": [
    "lr_clf = logistic_regression_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "[Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html) are similar to similar to logistic regression in the way that they also form a linear boundry to make predictions. The main difference is that SVMs find the samples hardest to classify, called the support vectors, and then finds the optimal boundery between them. SVMs are generally seen has a competitive algorithm that performs well on a variety of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def support_vector_machine_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    svm_clf = sk.svm.SVC(probability=True)\n",
    "    svm_clf.fit (X_train, y_train)\n",
    "    print(svm_clf.score (X_val, y_val))\n",
    "    \n",
    "    return svm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8089887640449438\n"
     ]
    }
   ],
   "source": [
    "svm_clf = support_vector_machine_clf(X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive bayes are a collection algorithms based on [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem). What they have incommon is that they make the naive assumption that all features are independent of each other. The features are then given a probability which is then combined for the final output. The algorithm used here is [Gaussian Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf.fit (X_train, y_train)\n",
    "    print(nb_clf.score (X_val, y_val))\n",
    "    \n",
    "    return nb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.702247191011236\n"
     ]
    }
   ],
   "source": [
    "nb_clf = naive_bayes_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor\n",
    "[K-Nearest Neighbor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) is one of the simpler algorithms. A prediction is made by finding the classification of the k most similar training examples (neighbors). They are then counted as votes as to how new data should be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_neighbors_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=6)\n",
    "    knn_clf.fit (X_train, y_train)\n",
    "    print(knn_clf.score (X_val, y_val))\n",
    "    \n",
    "    return knn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848314606741573\n"
     ]
    }
   ],
   "source": [
    "knn_clf = k_neighbors_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble Vote\n",
    "We have already used two ensamble algorithms, random forest and gradient boosting. What separates [this ensamble](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) is that, unlike the previous, it does not use it's own algorithm to make predictions. Instead it uses a collective of other algorithms which then cast votes to decide on a classification.\n",
    "\n",
    "Ensambles are used because they can reduce overfitting by having a regularizing effect. It is not necessary to use different algorithms, sometimes you use it on identical models that have been trained seperately. Just make sure that the model has a random element to them that can be varied for each model. You don't want a vote where everyone votes the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensamble_voting_clf(clfs, X_train, y_train, X_val, y_val, cv=20, score_individually=False):\n",
    "    \n",
    "    e_clf = ske.VotingClassifier(estimators=clfs, voting='hard') # Hard voting where majority rules\n",
    "    e_clf.fit (X_train, y_train)\n",
    "    \n",
    "    if score_individually:\n",
    "        for label, clf in clfs:\n",
    "            scores = cross_val_score(clf, X_val, y_val, cv=cv, scoring='accuracy')\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))    \n",
    "    \n",
    "    scores = cross_val_score(e_clf, X_val, y_val, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Voting Ensamble')) \n",
    "    \n",
    "    return e_clf    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building The Ensamble\n",
    "Here we will use all our algorithms for the vote. For optimal performance this might not be the wisest choice. For instance there is little point in keeping the decision tree since it is already included in random forest. But we will also use this to compare our different models performance on the validation. Noteworthy is that we will use ensambles in an ensamble, creating ensambleception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82 (+/- 0.13) [Decision Tree]\n",
      "Accuracy: 0.82 (+/- 0.11) [Random Forest]\n",
      "Accuracy: 0.84 (+/- 0.12) [Gradiant Boosting]\n",
      "Accuracy: 0.83 (+/- 0.11) [Logistic Regression]\n",
      "Accuracy: 0.80 (+/- 0.10) [SVM]\n",
      "Accuracy: 0.70 (+/- 0.10) [Naive Bayes]\n",
      "Accuracy: 0.81 (+/- 0.13) [K-Nearest]\n",
      "Accuracy: 0.83 (+/- 0.10) [Voting Ensamble]\n"
     ]
    }
   ],
   "source": [
    "clfs = ([('Decision Tree', dt_clf), ('Random Forest', rf_clf), ('Gradiant Boosting', gb_clf), \n",
    "         ('Logistic Regression', lr_clf), ('SVM', svm_clf), ('Naive Bayes', nb_clf), ('K-Nearest', knn_clf)])\n",
    "\n",
    "e_clf = ensamble_voting_clf(clfs, X_train, y_train, X_val, y_val, score_individually=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Now that we have our models fully trained we can move on to prediction on the test set. From the validation summary on our ensamble we see that we have similar performance on all algorithms except Naive Bayes which got horrible results. But we won't know which one will be best with out trying them out.\n",
    "\n",
    "Side note: Remember that Naive Bayes assumes that our features are independent. So for Naive Bayes to work better we should probably be more focused on eliminating/combining highly correlated features such as Pclass and Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(df, clf, export_path):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Makes predictions X -> y and exports to csv\n",
    "\n",
    "    Arguments:\n",
    "    df -- Data to predict from, pandas DataFrame\n",
    "    clf -- classifier, Classifier, sklearn classifier object\n",
    "    export_path -- Path and name of file, String\n",
    "        \n",
    "    Returns:\n",
    "    df_pred -- prediction, pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract Ids\n",
    "    y1 = df['PassengerId'].values\n",
    "    \n",
    "    # Make predictions\n",
    "    df_process = preprocess_dataframe(df, prediction_data=True, print_info=False)\n",
    "    X = df_process.values\n",
    "    y2 = clf.predict(X)\n",
    "    \n",
    "    # Combine ids and predictions\n",
    "    y = np.column_stack((y1, y2))\n",
    "    \n",
    "    # Restore pandas df\n",
    "    df_pred = pd.DataFrame(y)\n",
    "    df_pred.columns = [\"PassengerId\", \"Survived\"]\n",
    "    \n",
    "    # Export\n",
    "    df_pred.to_csv(export_path, sep=',', index=False)\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "df_test = predict(df_test, svm_clf, 'C:/GitHub/kaggle/titanic/predictions/predictions_svm.csv')\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Hyperparameters are the parameters in a model that are not found during training/fitting. An example is setting a max depth on a decision tree. These parameters still play a role in performance and is therefore a subject for optimization. This can be done by hand but usually it is best to automate it. There are two main ways of doing it, and we will try them both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporting\n",
    "First we build a report model that can tell us the results of our automated hypermeter tuning candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=1):\n",
    "    \n",
    "    for i in range(1, n_top + 1):\n",
    "        \n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            \n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(\n",
    "                results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "Random search is the first method for automatically finding good candidates of hyperparameters. It works by providing the search with a distribution of values for the hyperparameters. Then the search tests random sets of values within the given distribution over and over again. The test are then evalutated using an internal cross validation in order to rank them individually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_search_hyperparameters(clf, hyper_param, X_train, y_train, X_val, y_val, print_result=False):\n",
    "    \n",
    "    # Run randomized search\n",
    "    n_iter_search = 50\n",
    "    rnd_clf = RandomizedSearchCV(clf, param_distributions=hyper_param, n_iter=n_iter_search)\n",
    "\n",
    "    start = time()\n",
    "    rnd_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Print metrics \n",
    "    if print_result:\n",
    "        \n",
    "        print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "              \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "        print()\n",
    "        report(rnd_clf.cv_results_)\n",
    "        y_act, y_pred = y_val, clf.predict(X_val)\n",
    "        scores = cross_val_score(clf, X_val, y_val, cv=20, scoring='accuracy')\n",
    "        print(\"Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "        print()\n",
    "        print(classification_report(y_act, y_pred))\n",
    "       \n",
    "    return rnd_clf.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Grid search works in a similar way to random search. The main difference is that it uses a more systematic approach. Instead of a distribution it uses a grid of all the values that should be tested. Then it uses bruteforce to test and validate every possible combination in that grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_hyperparameters(clf, hyper_param, X_train, y_train, X_val, y_val, print_result=False):\n",
    "    \n",
    "    # Run grid search\n",
    "    grid_clf = GridSearchCV(clf, param_grid=hyper_param)\n",
    "    \n",
    "    start = time()\n",
    "    grid_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Print metrics \n",
    "    if print_result:\n",
    "        \n",
    "        print(\"GridSearchCV took %.2f seconds for %d candidates\"\n",
    "              \" parameter settings.\" % ((time() - start), len(grid_clf.cv_results_[\"rank_test_score\"])))\n",
    "        print()\n",
    "        report(grid_clf.cv_results_)\n",
    "        y_act, y_pred = y_val, clf.predict(X_val)\n",
    "        scores = cross_val_score(clf, X_val, y_val, cv=20, scoring='accuracy')\n",
    "        print(\"Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "        print()\n",
    "        print(classification_report(y_act, y_pred))\n",
    "    \n",
    "    return grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing The Models\n",
    "Hyperparameter tuning is where computing can start be a bit more heavy even for conventional machine learning algorithms. I found that the best results in my earlier predictions on the test set were usually made by Random Forest or SVM models. From now on I will focus on them.\n",
    "\n",
    "The method of finding the best parameters will be to start with wide ranges which then can be narrowed down with increased resolution. The initial values will be values that I have seen other people use. For grid search a good way of finding values is to increase the parameter with a factor ~3 for each step, i.e. [1, 3, 10, 30, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 22.74 seconds for 50 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.830 (std: 0.012)\n",
      "Parameters: {'min_samples_split': 2, 'max_features': 13, 'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 6}\n",
      "\n",
      "Validation Accuracy: 0.85 (+/- 0.11)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.86      0.86       113\n",
      "          1       0.75      0.75      0.75        65\n",
      "\n",
      "avg / total       0.82      0.82      0.82       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "hyper_param = {\"max_depth\": [3, None],\n",
    "               \"max_features\": stats.randint(3, X_train.shape[1]),\n",
    "               \"min_samples_split\": stats.randint(2, 11),\n",
    "               \"min_samples_leaf\": stats.randint(1, 11),\n",
    "               \"bootstrap\": [True, False],\n",
    "               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rf_rnd_clf = random_search_hyperparameters(rf_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 129.37 seconds for 216 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.820 (std: 0.005)\n",
      "Parameters: {'min_samples_split': 2, 'max_features': 14, 'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "\n",
      "Validation Accuracy: 0.82 (+/- 0.09)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.85      0.86       113\n",
      "          1       0.75      0.77      0.76        65\n",
      "\n",
      "avg / total       0.82      0.82      0.82       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "hyper_param = {\"max_depth\": [3, None],\n",
    "               \"max_features\": [3, 10, X_train.shape[1]],\n",
    "               \"min_samples_split\": [2, 3, 10],\n",
    "               \"min_samples_leaf\": [1, 3, 10],\n",
    "               \"bootstrap\": [True, False],\n",
    "               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rf_grid_clf = grid_search_hyperparameters(rf_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 25.72 seconds for 50 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.823 (std: 0.004)\n",
      "Parameters: {'C': 174, 'gamma': 0.007983889561916313, 'kernel': 'rbf'}\n",
      "\n",
      "Validation Accuracy: 0.80 (+/- 0.10)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.87      0.86       113\n",
      "          1       0.76      0.74      0.75        65\n",
      "\n",
      "avg / total       0.82      0.82      0.82       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "hyper_param = {'C': stats.randint(10, 1000),\n",
    "               'gamma' : stats.uniform(0.001, 1),\n",
    "               'kernel': ['rbf']}\n",
    "\n",
    "svm_rnd_clf = random_search_hyperparameters(svm_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 13.78 seconds for 30 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.819 (std: 0.006)\n",
      "Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.819 (std: 0.013)\n",
      "Parameters: {'C': 3000, 'gamma': 0.003, 'kernel': 'rbf'}\n",
      "\n",
      "Validation Accuracy: 0.81 (+/- 0.10)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.87      0.85       113\n",
      "          1       0.75      0.71      0.73        65\n",
      "\n",
      "avg / total       0.81      0.81      0.81       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "hyper_param = {'C': [1, 3, 10, 100, 3000, 1000], \n",
    "               'gamma' : [0.001,0.003, 0.01, 0.1, 1], \n",
    "               'kernel': ['rbf']}\n",
    "\n",
    "svm_grid_clf = grid_search_hyperparameters(svm_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Final Predictions\n",
    "Now we have completed all the steps necessary to make our final predictions. Looking at the output of our parameter searches we seem that the results seem to be similar for both random and grid search for both algorithms. They basically do the same things, and which one to use is mostly a matter of preference even though there is some debate about the subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "df_test = predict(df_test, svm_grid_clf, 'C:/GitHub/kaggle/titanic/predictions/predictions_tuned_svm.csv')\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(paramters):\n",
    "    \n",
    "    num_features = parameters['num_features']\n",
    "    X = tf.placeholder(tf.float32, [None, num_features], name='X')\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='y')\n",
    "\n",
    "    layers_dim = paramters['layers_dim']\n",
    "    \n",
    "    fc = tf.contrib.layers.stack(X, tf.contrib.layers.fully_connected, layers_dim)\n",
    "    Z = tf.contrib.layers.fully_connected(fc, 1, activation_fn=None, scope='Z')\n",
    "    \n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z, labels=y, name='Loss')\n",
    "    cost = tf.reduce_mean(loss, name='Cost')\n",
    "    \n",
    "    learning_rate = parameters['learning_rate']\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    prediction = tf.round(tf.sigmoid(Z))\n",
    "    correct_prediction = tf.equal(prediction, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    model = {'X': X, 'y': y, 'Z': Z, 'cost': cost,\n",
    "             'train_op': train_op, 'prediction': prediction, 'accuracy': accuracy}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X_train, Y_train, mini_batch_size = 32, seed = 0):\n",
    " \n",
    "    # Shuffle with identical seed to get same shuffle in both X and Y\n",
    "    np.random.seed(seed)\n",
    "    X_train = np.random.permutation(X_train)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    Y_train = np.random.permutation(Y_train)\n",
    "    \n",
    "    m = X_train.shape[0]\n",
    "    num_batches = int(m / mini_batch_size)\n",
    "    \n",
    "    # Split data into smaller batches for ready for stochastic gradient descent\n",
    "    minibatches_X = np.array_split(X_train, num_batches)\n",
    "    minibatches_Y = np.array_split(Y_train, num_batches)\n",
    "    \n",
    "    minibatches = zip(minibatches_X, minibatches_Y)\n",
    "    \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(parameters, model):\n",
    "    \n",
    "    num_epochs = parameters['num_epochs']\n",
    "    minibatch_size = parameters['minibatch_size']\n",
    "    X_train = parameters['X_train']\n",
    "    y_train = parameters['y_train']\n",
    "    \n",
    "    \n",
    "    train_size = X_train.shape[0]\n",
    "    saver = tf.train.Saver()\n",
    "    epoch_list = []\n",
    "    cost_list = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(train_size / minibatch_size)\n",
    "            minibatches = random_mini_batches(X_train, y_train, minibatch_size)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                (minibatch_X, minibatch_y) = minibatch\n",
    "                minibatch_y = np.reshape(minibatch_y, (minibatch_X.shape[0], 1))\n",
    "                feed_dict = {model['X'] : minibatch_X, model['y'] : minibatch_y}\n",
    "\n",
    "                _model ,minibatch_cost = sess.run([model['train_op'], model['cost']], feed_dict= feed_dict)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "            if parameters['print'] and (epoch % parameters['print_freq'] == 0):\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            \n",
    "            if parameters['save_cost'] and (epoch % parameters['save_cost_freq'] == 0):\n",
    "                epoch_list.append(epoch)\n",
    "                cost_list.append(epoch_cost)\n",
    "                \n",
    "        saver.save(sess, parameters['save_path'])\n",
    "        \n",
    "    return {'epoch_list': epoch_list, 'cost_list' : cost_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "\n",
    "# set model parameters\n",
    "parameters['X_train'] = X_train\n",
    "parameters['y_train'] = y_train\n",
    "parameters['X_val'] = X_val\n",
    "parameters['y_val'] = y_val\n",
    "parameters['X_test'] = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "parameters['PassangerId'] = pd.DataFrame(parameters['X_test']['PassengerId'], columns=['PassengerId'])\n",
    "parameters['layers_dim'] = [14]\n",
    "parameters['num_features'] = X_train.shape[1]\n",
    "parameters['layers_dim'] = [14]\n",
    "parameters['learning_rate'] = 0.01\n",
    "\n",
    "# set train parameters (hyper parameter)\n",
    "parameters['num_epochs'] = 3000\n",
    "parameters['minibatch_size'] = 20\n",
    "\n",
    "# set option parameters\n",
    "parameters['model_name'] = 'nn_clf'\n",
    "parameters['save_path'] = 'C:/GitHub/kaggle/titanic/models/' + parameters['model_name']\n",
    "parameters['print'] = True\n",
    "parameters['print_freq'] = 500\n",
    "parameters['save_cost'] = True\n",
    "parameters['save_cost_freq'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Cost after epoch 0: 0.583991\n",
      "Cost after epoch 500: 0.338940\n",
      "Cost after epoch 1000: 0.322726\n",
      "Cost after epoch 1500: 0.319052\n",
      "Cost after epoch 2000: 0.316830\n",
      "Cost after epoch 2500: 0.314657\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = create_model(parameters)\n",
    "    plot_data = train_model(parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZlmFPwiItPM/wdQru6GdFRb7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d6776a00b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print\n",
    "if parameters['save_cost']:\n",
    "    plt.plot(plot_data['epoch_list'], plot_data['cost_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(parameters, model):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, parameters['save_path'])\n",
    "        print (\"Train Accuracy:\", model['accuracy'].eval({model['X']: X_train, \n",
    "                                                          model['y']: np.reshape(y_train, (X_train.shape[0], 1)) }))\n",
    "        print (\"Validation Accuracy:\", model['accuracy'].eval({model['X']: X_val, \n",
    "                                                          model['y']: np.reshape(y_val, (X_val.shape[0], 1)) }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/GitHub/kaggle/titanic/models/nn_clf\n",
      "Train Accuracy: 0.8737728\n",
      "Validation Accuracy: 0.8202247\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = create_model(parameters)\n",
    "    evaluate(parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_nn(parameters, model):\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    X_val = parameters['X_val']\n",
    "    X_val = preprocess_dataframe(X_val, prediction_data=True, print_info=False)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, parameters['save_path'])\n",
    "        prediction = model['prediction'].eval({model['X']: X_val})\n",
    "        return prediction.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/GitHub/kaggle/titanic/models/nn_clf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         1\n",
       "3          895         1\n",
       "4          896         0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = parameters['PassangerId']\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    model = create_model(parameters)\n",
    "    y_test = predict_nn(parameters, model)\n",
    "    df_test['Survived'] = y_test\n",
    "    df_test.to_csv('C:/GitHub/kaggle/titanic/predictions/predictions_neural_network.csv', sep=',', index=False)\n",
    "\n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Passanger Survival Prediction\n",
    "\n",
    "This notebook contains my efforts and thoughts for the Titanic: Machine Learning from Disaster [Kaggle](https://www.kaggle.com/c/titanic) competition. The aim is to explore different machine learning concepts rather than to find the optimal score. It is also a practice in making a well documented and easy to follow notebook.\n",
    "\n",
    "### Covered topics\n",
    "- Data preprocessing using Pandas\n",
    "- Feature engineering\n",
    "- Conventional ML models using scikit-learn\n",
    "- Ensambles\n",
    "- Hyperparameter tuning\n",
    "- Neural network using Tensorflow\n",
    "\n",
    "### Things for another day\n",
    "- Visualizations\n",
    "- Regularization\n",
    "- Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import math\n",
    "from time import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removes a warning in sklearn that will be fixed during an update mid 2018\n",
    "import warnings\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "    le = sk.preprocessing.LabelEncoder()\n",
    "    le.fit([1, 2, 2, 6])\n",
    "    le.transform([1, 1, 2, 6])\n",
    "    le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "We will use the popular [Pandas](https://pandas.pydata.org/) library to preprocess our data. We start with reading the data from .csv with read_csv(). When we have our dataframe in memory we can use head to take a first look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/GitHub/kaggle/titanic/data/train.csv', sep=',', header=0)\n",
    "print('Data size: ' + str(df.shape))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the data\n",
    "Now we can inspect our features. We have 11 feauters, which we will call X, and 1 output (Survived), which we will call y. We can also see that we have 891 training examples. Now let's go over our features and try to reason over them. The most important thing is to find data that we think might affect the chance of survival for a passenger.\n",
    "\n",
    "#### PassangerId\n",
    "This is just used for indentification when scoring the predictions. It does not contain any ground truth about passengers and their survival and should therefore be removed before training.\n",
    "\n",
    "#### Survived\n",
    "This is what we want to predict, our output y. It should be split from the features and kept separate.\n",
    "\n",
    "#### Pclass\n",
    "The fare class of the passenger. Pclass = 1 is first class, Pclass = 2 is second class and Pclass = 3 is third class. This feature will likely affect survival. Things such people density and the distribution of [life boats](https://en.wikipedia.org/wiki/Lifeboats_of_the_RMS_Titanic) will likely vary amoung classes and play a part in survival.\n",
    "\n",
    "#### Name\n",
    "There are two things with the name that might be interesting in terms of surivival. The name might give us the gender of the passenger. We can however see that the gender is already provided in another feature, so there is no point in extracting it here. The second, and more interesting thing, is the title. The title can tell us the marital and sociatal status of a passenger.\n",
    "\n",
    "#### Sex\n",
    "[\"Women and children first\"](https://en.wikipedia.org/wiki/Women_and_children_first) is famously associated with the Titanic. Therefore it is safe to assume that sex will play a large factor in survival.\n",
    "\n",
    "#### Age\n",
    "The same motivation as with sex can be applied as to why age would be important for survival.\n",
    "\n",
    "#### SibSp\n",
    "Number of siblings/spouses onboard. Probably will be a factor for survival since family members can help eachother.\n",
    "\n",
    "#### Parch\n",
    "Number of parents/children onboard. Very similar to the feature above.\n",
    "\n",
    "#### Ticket\n",
    "The tickets numbers seem to have some useful information in them. However, there seem to be little structure in the data. I decided to skip it to minimize initial effort. Might revise it later.\n",
    "\n",
    "#### Fare\n",
    "The price of the ticket. Will likely have similar factors as Pclass.\n",
    "\n",
    "#### Embarked\n",
    "Port of embarkation. Where C = Cherbourg, Q = Queenstown and S = Southampton. This should, at first glance, not play a big role in survival. However, it could possibly be a indirect factor and should not be dismissed without investigation.\n",
    "\n",
    "### Data size\n",
    "891 samples of data is on the low end for machine learning. It will mean that the model might be subseptible to overfitting. Overfitting is when to model specializes too much on the given dataset instead of learning a ground truth which that can help with predicting previously unseen data. It also means that it might be reasonable to chose simpler, conventional, machine learning models instead of the more data hungry neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils\n",
    "Below are just some util functions that will later be used for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checks and returns if a string contains one of a list of given substrings.\n",
    "def substrings_in_string(whole, subs): \n",
    "    \n",
    "    for x in subs:\n",
    "        if x in str(whole): \n",
    "            return x\n",
    "        \n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reducing data by combining decks with similar survival rates\n",
    "def reduce_decks(x):\n",
    "    \n",
    "    reduced_decks = {\n",
    "        \"A\": \"AG\",\n",
    "        \"B\": \"BDE\",\n",
    "        \"C\": \"CF\",\n",
    "        \"D\": \"BDE\",\n",
    "        \"E\": \"BDE\",\n",
    "        \"F\": \"CF\",\n",
    "        \"G\": \"AG\",\n",
    "        \"T\": \"UT\",\n",
    "        \"Unknown\": \"UT\"}\n",
    "    \n",
    "    return reduced_decks[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract title from a name\n",
    "def extract_title(full_name):\n",
    "    \n",
    "    full_name  = str(full_name)\n",
    "    \n",
    "    x = full_name.split(\", \")\n",
    "    x = x[1]\n",
    "    x = x.split('.')\n",
    "    title = x[0]\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplify titles into fewer more significant titles\n",
    "def simplify_titles(x):\n",
    "    \n",
    "    # Female aristocrate -> Mrs since married females have higher survival rate than aristocrats in general\n",
    "    # There are very few aristocrats, but they seem to roughly survive in the same rate as masters\n",
    "    \n",
    "    titles = {\n",
    "    \"Capt\": \"Officer\",\n",
    "    \"Col\": \"Officer\",\n",
    "    \"Major\": \"Officer\",\n",
    "    \"Jonkheer\": \"Officer\",\n",
    "    \"Don\": \"Aristocrat/Master\",\n",
    "    \"Dona\": \"Mrs\",\n",
    "    \"Sir\" : \"Aristocrat/Master\",\n",
    "    \"Dr\": \"Officer\",\n",
    "    \"Rev\": \"Officer\",\n",
    "    \"the Countess\":\"Mrs\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Mrs\",\n",
    "    \"Mr\" : \"Mr\",\n",
    "    \"Mrs\" : \"Mrs\",\n",
    "    \"Miss\" : \"Miss\",\n",
    "    \"Master\" : \"Aristocrat/Master\",\n",
    "    \"Lady\" : \"Mrs\"\n",
    "    }\n",
    "    \n",
    "    return titles[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and feature engineering\n",
    "Data preprocessing and feature engineering go hand in hand with the focus to find the best possible data for a model. They are together probably the most important parts for a successful, especially when the data size is small. It is not something that is performed once and then forgotten. Data should be revised and optimized in iterations as new models are tested.\n",
    "\n",
    "### Data preprocessing\n",
    "Data preprocessing is when data is adjusted to be recieved by a model. The following processing is performed:\n",
    "- Removing noise - Data not relevant for the prediction will make the model more noisy and should be removed.\n",
    "- Missing values - Values that are missing are filled with mean of most common occuring value.\n",
    "- Normalization - If data on different scales will serve as initial weights for the model. Therefor it is good to normalize the data and let the model find the weights be itself. A good normalization is to have all data on a magnititude of 0.0 - 1.0.\n",
    "- Data discretation - Data that represents different categories (such as Pclass) can be represented as separate discreet features instead of quanties.\n",
    "\n",
    "### Feature engineering\n",
    "Feature engineering is when domain knowledge is applied to find new features that will be more useful for the model. The following techniques were used.\n",
    "- Indicator Variables - Quantities can be divided into meaningful brackets. Example: Age can be divided into categories to find children since we know that they are more likely to live.\n",
    "- Interaction Features - Features can sometimes be combined into something meaningful. Example: SibSp and Parch can be combined to find the family size.\n",
    "- Grouping sparse data - Categorical data that has low occurence can be grouped into larger meaningful categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, prediction_data=False, print_info=False):\n",
    "    # Age has missing values which is replaced with average\n",
    "    # Might also consider dividing age into classes of age brackets\n",
    "    df['Age'].fillna((df['Age'].mean()), inplace = True)\n",
    "    df['Fare'].fillna((df['Fare'].mean()), inplace = True)\n",
    "    \n",
    "    # Encode sex into binary (0 = male, 1 = female)\n",
    "    df['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n",
    "    \n",
    "    # Use the prefix in cabin to find the deck of the cabin\n",
    "    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n",
    "    df['Deck' ]= df['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "    df['Deck' ]= df['Deck'].map(lambda x: reduce_decks(x))\n",
    "    \n",
    "    # Calculate family size by combining SibSp and Parch\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "    df['FamilySize'] = df['FamilySize'].apply(lambda x: 'Alone' if x == 0 else ('Small' if x > 3 else 'Large'))\n",
    "\n",
    "    # Divide age into useful brackets\n",
    "    df['Child'] = df['Age'].apply(lambda x: 1 if x <= 13 else 0)\n",
    "    \n",
    "    # Extract and process titles\n",
    "    df['Title']= df['Name'].map(lambda x: extract_title(x))\n",
    "    if not prediction_data:\n",
    "        unique_titles = df['Title'].unique()\n",
    "        survival_by_title = df.groupby('Title').mean()['Survived']\n",
    "    \n",
    "    df['Title'] = np.where(((df['Title'] == 'Dr') & (df['Sex'] == 1)), 'Mrs', df['Title'])\n",
    "    \n",
    "    df['Title']= df['Title'].map(lambda x: simplify_titles(x))\n",
    "    if not prediction_data:\n",
    "        unique_titles_simplified = df['Title'].unique()\n",
    "        survival_by_title_simplified = df.groupby('Title').mean()['Survived']        \n",
    "    \n",
    "    # Gather info on the significance of these classes for survival\n",
    "    if not prediction_data:\n",
    "        survival_by_plcass = df.groupby('Pclass').mean()['Survived']\n",
    "        survival_by_deck = df.groupby('Deck').mean()['Survived']\n",
    "        survival_by_embark = df.groupby('Embarked').mean()['Survived']\n",
    "        survival_by_family = df.groupby('FamilySize').mean()['Survived']\n",
    "        survival_by_child = df.groupby('Child').mean()['Survived']\n",
    "    \n",
    "    # Split classes with one hot encoding\n",
    "    # Pclass   - splits columns into (1 = Pclass_1, 2 = Pclass_2, 3 = Pclass_3)\n",
    "    # Embarked - splits columns into (C = Embarked_C, Q = Embarked_Q, S = Embarked_S)\n",
    "    # Deck     - splits columns into decks with letters\n",
    "    # Title    - splits columns titles\n",
    "    df = pd.get_dummies(df, columns = ['Pclass', 'Embarked', 'Deck', 'Title', 'FamilySize'])\n",
    "\n",
    "    # Drop columns with data deemed not relevant for learning\n",
    "    # Name   - Replaced by Title\n",
    "    # Ticket - Ticket does not really say much, price and class are already included which says the most\n",
    "    # Cabin  - Replaced by Deck\n",
    "    # Age    - Replaced by Child\n",
    "    # SibSp  - Replaced by FamilySize\n",
    "    # Parch  - Replaced by FamilySize\n",
    "    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Age', 'SibSp', 'Parch'], axis=1)\n",
    "    \n",
    "    # Normalize the data\n",
    "    norm_vals = ['Fare'] #, 'FamilySize']\n",
    "    df[norm_vals] = (df[norm_vals] - df[norm_vals].min())/(df[norm_vals].max() - df[norm_vals].min())\n",
    "    \n",
    "    # Look at interesting metrics to find information about the preprocessing/feature engineering\n",
    "    if print_info:\n",
    "        if not prediction_data:\n",
    "            print('--------------------------------------------------------------------------------------')\n",
    "            print('SURVIVAL RATE')\n",
    "            print('--------------------------------------------------------------------------------------')\n",
    "            print('Overall survival rate: ' + str(df['Survived'].mean()))\n",
    "            print()\n",
    "            print(survival_by_plcass.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_embark.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_deck.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_title.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_title_simplified.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_family.sort_values(ascending=False))\n",
    "            print()\n",
    "            print(survival_by_child.sort_values(ascending=False))\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('TITLES')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('All titels: ')\n",
    "        print(unique_titles)\n",
    "        print()\n",
    "        print('Simplied titels: ')\n",
    "        print(unique_titles_simplified)\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('SUMS')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.sum())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('DATA INFO')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.info())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print('MISSING VALUES')\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        print(df.isnull().sum())\n",
    "        print('--------------------------------------------------------------------------------------')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "To score the model it is important that the scoring is not performed on data that also has been used to train the model. Otherwise the evaluation will be overly optimistic but the performance on independant data will most likely perform much worse. The solution is to exclude a fraction of the data during training so that it can later be used for evaluation. This set is usually called the cross validation set.\n",
    "\n",
    "Validation does however don't stop after training. And every time data has been used to make a decision then new data has to be used to evaluate how well that decision will hold up on new data. A best practice is to further divide the data into train/test/cross validation.\n",
    "\n",
    "- Train set - Data used to train the model\n",
    "- Cross validation - Data used for finding deciding between different models and hyperparamteres\n",
    "- Test - Data used to make a final evaluation on how the model will perform on unseen data\n",
    "\n",
    "In this competition the test set is already set aside by default, so only a simply train/validation split is performed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into train/validation set\n",
    "def split_data(df):\n",
    "    df_train = df.sample(frac = 0.8, random_state = 42)\n",
    "    df_val = df.drop(df_train.index)  \n",
    "    \n",
    "    X_train = df_train.drop(['Survived'], axis=1).values\n",
    "    y_train = df_train['Survived'].values\n",
    "    \n",
    "    X_val = df_val.drop(['Survived'], axis=1).values\n",
    "    y_val = df_val['Survived'].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "SURVIVAL RATE\n",
      "--------------------------------------------------------------------------------------\n",
      "Overall survival rate: 0.3838383838383838\n",
      "\n",
      "Pclass\n",
      "1    0.629630\n",
      "2    0.472826\n",
      "3    0.242363\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Embarked\n",
      "C    0.553571\n",
      "Q    0.389610\n",
      "S    0.336957\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Deck\n",
      "BDE    0.752212\n",
      "CF     0.591549\n",
      "AG     0.473684\n",
      "UT     0.299419\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Title\n",
      "the Countess    1.000000\n",
      "Mlle            1.000000\n",
      "Lady            1.000000\n",
      "Ms              1.000000\n",
      "Sir             1.000000\n",
      "Mme             1.000000\n",
      "Mrs             0.792000\n",
      "Miss            0.697802\n",
      "Master          0.575000\n",
      "Major           0.500000\n",
      "Col             0.500000\n",
      "Dr              0.428571\n",
      "Mr              0.156673\n",
      "Rev             0.000000\n",
      "Jonkheer        0.000000\n",
      "Don             0.000000\n",
      "Capt            0.000000\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Title\n",
      "Mrs                  0.800000\n",
      "Miss                 0.701087\n",
      "Aristocrat/Master    0.571429\n",
      "Officer              0.222222\n",
      "Mr                   0.156673\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "FamilySize\n",
      "Large    0.578767\n",
      "Alone    0.303538\n",
      "Small    0.161290\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Child\n",
      "1    0.591549\n",
      "0    0.365854\n",
      "Name: Survived, dtype: float64\n",
      "--------------------------------------------------------------------------------------\n",
      "TITLES\n",
      "--------------------------------------------------------------------------------------\n",
      "All titels: \n",
      "['Mr' 'Mrs' 'Miss' 'Master' 'Don' 'Rev' 'Dr' 'Mme' 'Ms' 'Major' 'Lady'\n",
      " 'Sir' 'Mlle' 'Col' 'Capt' 'the Countess' 'Jonkheer']\n",
      "\n",
      "Simplied titels: \n",
      "['Mr' 'Mrs' 'Miss' 'Aristocrat/Master' 'Officer']\n",
      "--------------------------------------------------------------------------------------\n",
      "SUMS\n",
      "--------------------------------------------------------------------------------------\n",
      "Survived                   342.000000\n",
      "Sex                        314.000000\n",
      "Fare                        56.006859\n",
      "Child                       71.000000\n",
      "Pclass_1                   216.000000\n",
      "Pclass_2                   184.000000\n",
      "Pclass_3                   491.000000\n",
      "Embarked_C                 168.000000\n",
      "Embarked_Q                  77.000000\n",
      "Embarked_S                 644.000000\n",
      "Deck_AG                     19.000000\n",
      "Deck_BDE                   113.000000\n",
      "Deck_CF                     71.000000\n",
      "Deck_UT                    688.000000\n",
      "Title_Aristocrat/Master     42.000000\n",
      "Title_Miss                 184.000000\n",
      "Title_Mr                   517.000000\n",
      "Title_Mrs                  130.000000\n",
      "Title_Officer               18.000000\n",
      "FamilySize_Alone           537.000000\n",
      "FamilySize_Large           292.000000\n",
      "FamilySize_Small            62.000000\n",
      "dtype: float64\n",
      "--------------------------------------------------------------------------------------\n",
      "DATA INFO\n",
      "--------------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 22 columns):\n",
      "Survived                   891 non-null int64\n",
      "Sex                        891 non-null int64\n",
      "Fare                       891 non-null float64\n",
      "Child                      891 non-null int64\n",
      "Pclass_1                   891 non-null uint8\n",
      "Pclass_2                   891 non-null uint8\n",
      "Pclass_3                   891 non-null uint8\n",
      "Embarked_C                 891 non-null uint8\n",
      "Embarked_Q                 891 non-null uint8\n",
      "Embarked_S                 891 non-null uint8\n",
      "Deck_AG                    891 non-null uint8\n",
      "Deck_BDE                   891 non-null uint8\n",
      "Deck_CF                    891 non-null uint8\n",
      "Deck_UT                    891 non-null uint8\n",
      "Title_Aristocrat/Master    891 non-null uint8\n",
      "Title_Miss                 891 non-null uint8\n",
      "Title_Mr                   891 non-null uint8\n",
      "Title_Mrs                  891 non-null uint8\n",
      "Title_Officer              891 non-null uint8\n",
      "FamilySize_Alone           891 non-null uint8\n",
      "FamilySize_Large           891 non-null uint8\n",
      "FamilySize_Small           891 non-null uint8\n",
      "dtypes: float64(1), int64(3), uint8(18)\n",
      "memory usage: 43.6 KB\n",
      "None\n",
      "--------------------------------------------------------------------------------------\n",
      "MISSING VALUES\n",
      "--------------------------------------------------------------------------------------\n",
      "Survived                   0\n",
      "Sex                        0\n",
      "Fare                       0\n",
      "Child                      0\n",
      "Pclass_1                   0\n",
      "Pclass_2                   0\n",
      "Pclass_3                   0\n",
      "Embarked_C                 0\n",
      "Embarked_Q                 0\n",
      "Embarked_S                 0\n",
      "Deck_AG                    0\n",
      "Deck_BDE                   0\n",
      "Deck_CF                    0\n",
      "Deck_UT                    0\n",
      "Title_Aristocrat/Master    0\n",
      "Title_Miss                 0\n",
      "Title_Mr                   0\n",
      "Title_Mrs                  0\n",
      "Title_Officer              0\n",
      "FamilySize_Alone           0\n",
      "FamilySize_Large           0\n",
      "FamilySize_Small           0\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------------\n",
      "X_train shape: (713, 21)\n",
      "y_train shape: (713,)\n",
      "X_val shape: (178, 21)\n",
      "y_val shape: (178,)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Child</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>...</th>\n",
       "      <th>Deck_CF</th>\n",
       "      <th>Deck_UT</th>\n",
       "      <th>Title_Aristocrat/Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Officer</th>\n",
       "      <th>FamilySize_Alone</th>\n",
       "      <th>FamilySize_Large</th>\n",
       "      <th>FamilySize_Small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex      Fare  Child  Pclass_1  Pclass_2  Pclass_3  Embarked_C  \\\n",
       "0         0    0  0.014151      0         0         0         1           0   \n",
       "1         1    1  0.139136      0         1         0         0           1   \n",
       "2         1    1  0.015469      0         0         0         1           0   \n",
       "3         1    1  0.103644      0         1         0         0           0   \n",
       "4         0    0  0.015713      0         0         0         1           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S        ...         Deck_CF  Deck_UT  \\\n",
       "0           0           1        ...               0        1   \n",
       "1           0           0        ...               1        0   \n",
       "2           0           1        ...               0        1   \n",
       "3           0           1        ...               1        0   \n",
       "4           0           1        ...               0        1   \n",
       "\n",
       "   Title_Aristocrat/Master  Title_Miss  Title_Mr  Title_Mrs  Title_Officer  \\\n",
       "0                        0           0         1          0              0   \n",
       "1                        0           0         0          1              0   \n",
       "2                        0           1         0          0              0   \n",
       "3                        0           0         0          1              0   \n",
       "4                        0           0         1          0              0   \n",
       "\n",
       "   FamilySize_Alone  FamilySize_Large  FamilySize_Small  \n",
       "0                 0                 1                 0  \n",
       "1                 0                 1                 0  \n",
       "2                 1                 0                 0  \n",
       "3                 0                 1                 0  \n",
       "4                 1                 0                 0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed = preprocess_dataframe(df, False, True)\n",
    "\n",
    "X_train, y_train, X_val, y_val = split_data(df_processed)\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_val shape: \" + str(X_val.shape))\n",
    "print (\"y_val shape: \" + str(y_val.shape))\n",
    "print()\n",
    "\n",
    "df_processed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Machine Learning Models\n",
    "Today the hype is all neural networks, but they are not strictly better and it is often better to start with conventional machine learning algorithms. They are faster to implement and can often give a better result. Neural networks are notouriously heavy on computation and usually require lots of data before they outperform conventional algorithms.\n",
    "\n",
    "In our case we have a very small training set, only 891 samples. Starting with conventional algorithms definately feels like the right choice. The strategy here will be to cast a wide net and try a wide variety of algorithms that can perform classification and then narrow it down to those who perform the best.\n",
    "\n",
    "To implement the models we use scikit-learn, which is more or less the defacto framework for conventional ML. All classifiers used here are not really top candidates, but since scikit-learn makes it so easy to implement, we will still use quite a few just to learn. The algorithms we will use are:\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- Gradient boosting\n",
    "- Logistic regression\n",
    "- Support vector machine\n",
    "- Naive Bayes\n",
    "- K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree \n",
    "[Decision Tree](http://scikit-learn.org/stable/modules/tree.html) is a simple algorithm. It builds a tree of nested if-then-else statements based on the input features in order to make a prediction. It is generally weak to overfitting when the ratio of data/features is low, which unfortunately is our case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decision_tree_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    dt_clf = sk.tree.DecisionTreeClassifier(max_depth=20)\n",
    "    dt_clf.fit (X_train, y_train)\n",
    "    print(dt_clf.score (X_val, y_val))\n",
    "    \n",
    "    return dt_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8033707865168539\n"
     ]
    }
   ],
   "source": [
    "dt_clf = decision_tree_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "[Random forest](http://scikit-learn.org/stable/modules/tree.html) is an ensamble algorithms. That means that it combines more than one instance of one or more algorithms in order to make a prediction. Random forest divides the data into smaller subsets and trains a decision tree on each set which is then averaged to make the final prediction. This helps counteract the overfitting that is seen in decision trees and is generally performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_clf(X_train, y_train, X_val, y_val):\n",
    "\n",
    "    rf_clf = ske.RandomForestClassifier(n_estimators=50)\n",
    "    rf_clf.fit (X_train, y_train)\n",
    "    print(rf_clf.score (X_val, y_val))\n",
    "    \n",
    "    return rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797752808988764\n"
     ]
    }
   ],
   "source": [
    "rf_clf = random_forest_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting \n",
    "[Gradient Boosting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) is another example of ensamble algorithms. It is a bit more involved than random forest and is not easy to summerize. You can say that it iteratively adds weaker models together weights them based on accuracy in order to output as one strong model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    gb_clf = ske.GradientBoostingClassifier(n_estimators=50)\n",
    "    gb_clf.fit (X_train, y_train)\n",
    "    print(gb_clf.score (X_val, y_val))\n",
    "    \n",
    "    return gb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8258426966292135\n"
     ]
    }
   ],
   "source": [
    "gb_clf = gradient_boosting_clf(X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "[Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is an algorithm where a linear boundary is created that divides the positive and negative predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit (X_train, y_train)\n",
    "    print(lr_clf.score (X_val, y_val))\n",
    "    \n",
    "    return lr_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8426966292134831\n"
     ]
    }
   ],
   "source": [
    "lr_clf = logistic_regression_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "[Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html) are similar to similar to logistic regression in the way that they also form a linear boundry to make predictions. The main difference is that SVMs find the samples hardest to classify, called the support vectors, and then finds the optimal boundery between them. SVMs are generally seen has a competitive algorithm that performs well on a variety of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def support_vector_machine_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    svm_clf = sk.svm.SVC(probability=True)\n",
    "    svm_clf.fit (X_train, y_train)\n",
    "    print(svm_clf.score (X_val, y_val))\n",
    "    \n",
    "    return svm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8370786516853933\n"
     ]
    }
   ],
   "source": [
    "svm_clf = support_vector_machine_clf(X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive bayes are a collection algorithms based on [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem). What they have incommon is that they make the naive assumption that all features are independent of each other. The features are then given a probability which is then combined for the final output. The algorithm used here is [Gaussian Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf.fit (X_train, y_train)\n",
    "    print(nb_clf.score (X_val, y_val))\n",
    "    \n",
    "    return nb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7921348314606742\n"
     ]
    }
   ],
   "source": [
    "nb_clf = naive_bayes_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor\n",
    "[K-Nearest Neighbor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) is one of the simpler algorithms. A prediction is made by finding the classification of the k most similar training examples (neighbors). They are then counted as votes as to how new data should be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_neighbors_clf(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=6)\n",
    "    knn_clf.fit (X_train, y_train)\n",
    "    print(knn_clf.score (X_val, y_val))\n",
    "    \n",
    "    return knn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8202247191011236\n"
     ]
    }
   ],
   "source": [
    "knn_clf = k_neighbors_clf(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble Vote\n",
    "We have already used two ensamble algorithms, random forest and gradient boosting. What separates [this ensamble](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) is that, unlike the previous, it does not use it's own algorithm to make predictions. Instead it uses a collective of other algorithms which then cast votes to decide on a classification.\n",
    "\n",
    "Ensambles can in some cases improve accuracy. It is not necessary to use different algorithms, sometimes you use it on identical models that have been trained seperately. Just make sure that the model has a random element to them that can be varied for each model. You don't want a vote where everyone votes the same...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensamble_voting_clf(clfs, X_train, y_train, X_val, y_val, cv=20, score_individually=False):\n",
    "    \n",
    "    e_clf = ske.VotingClassifier(estimators=clfs, voting='hard') # Hard voting where majority rules\n",
    "    e_clf.fit (X_train, y_train)\n",
    "    \n",
    "    if score_individually:\n",
    "        for label, clf in clfs:\n",
    "            scores = cross_val_score(clf, X_val, y_val, cv=cv, scoring='accuracy')\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))    \n",
    "    \n",
    "    scores = cross_val_score(e_clf, X_val, y_val, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Voting Ensamble')) \n",
    "    \n",
    "    return e_clf    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building The Ensamble\n",
    "Here we will use all our algorithms for the vote. For optimal performance this might not be the wisest choice. For instance there is little point in keeping the decision tree since it is already included in random forest. But we will also use this to compare our different models performance on the validation. Noteworthy is that we will use ensambles in an ensamble, creating ensambleception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82 (+/- 0.12) [Decision Tree]\n",
      "Accuracy: 0.84 (+/- 0.14) [Random Forest]\n",
      "Accuracy: 0.85 (+/- 0.13) [Gradiant Boosting]\n",
      "Accuracy: 0.84 (+/- 0.11) [Logistic Regression]\n",
      "Accuracy: 0.81 (+/- 0.10) [SVM]\n",
      "Accuracy: 0.82 (+/- 0.10) [Naive Bayes]\n",
      "Accuracy: 0.79 (+/- 0.12) [K-Nearest]\n",
      "Accuracy: 0.84 (+/- 0.08) [Voting Ensamble]\n"
     ]
    }
   ],
   "source": [
    "clfs = ([('Decision Tree', dt_clf), ('Random Forest', rf_clf), ('Gradiant Boosting', gb_clf), \n",
    "         ('Logistic Regression', lr_clf), ('SVM', svm_clf), ('Naive Bayes', nb_clf), ('K-Nearest', knn_clf)])\n",
    "\n",
    "e_clf = ensamble_voting_clf(clfs, X_train, y_train, X_val, y_val, score_individually=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Now that we have our models fully trained we can move on to prediction on the test set. From the validation summary on our ensamble we see that we have similar performance on all algorithms except Naive Bayes which got horrible results. But we won't know which one will be best with out trying them out.\n",
    "\n",
    "Side note: Remember that Naive Bayes assumes that our features are independent. So for Naive Bayes to work better we should probably be more focused on eliminating/combining highly correlated features such as Pclass and Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(df, clf, export_path):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Makes predictions X -> y and exports to csv\n",
    "\n",
    "    Arguments:\n",
    "    df -- Data to predict from, pandas DataFrame\n",
    "    clf -- classifier, Classifier, sklearn classifier object\n",
    "    export_path -- Path and name of file, String\n",
    "        \n",
    "    Returns:\n",
    "    df_pred -- prediction, pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract Ids\n",
    "    y1 = df['PassengerId'].values\n",
    "    \n",
    "    # Make predictions\n",
    "    df_process = preprocess_dataframe(df, prediction_data=True, print_info=False)\n",
    "    X = df_process.values\n",
    "    y2 = clf.predict(X)\n",
    "    \n",
    "    # Combine ids and predictions\n",
    "    y = np.column_stack((y1, y2))\n",
    "    \n",
    "    # Restore pandas df\n",
    "    df_pred = pd.DataFrame(y)\n",
    "    df_pred.columns = [\"PassengerId\", \"Survived\"]\n",
    "    \n",
    "    # Export\n",
    "    df_pred.to_csv(export_path, sep=',', index=False)\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "df_test = predict(df_test, svm_clf, 'C:/GitHub/kaggle/titanic/predictions/predictions_svm.csv')\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Hyperparameters are the parameters in a model that are not found during training/fitting. An example is setting a max depth on a decision tree. These parameters still play a role in performance and is therefore a subject for optimization. This can be done by hand but usually it is best to automate it. There are two main ways of doing it, and we will try them both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporting\n",
    "First we build a report model that can tell us the results of our automated hypermeter tuning candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=1):\n",
    "    \n",
    "    for i in range(1, n_top + 1):\n",
    "        \n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            \n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(\n",
    "                results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "Random search is the first method for automatically finding good candidates of hyperparameters. It works by providing the search with a distribution of values for the hyperparameters. Then the search tests random sets of values within the given distribution over and over again. The test are then evalutated using an internal cross validation in order to rank them individually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_search_hyperparameters(clf, hyper_param, X_train, y_train, X_val, y_val, print_result=False):\n",
    "    \n",
    "    # Run randomized search\n",
    "    n_iter_search = 100\n",
    "    rnd_clf = RandomizedSearchCV(clf, param_distributions=hyper_param, n_iter=n_iter_search)\n",
    "\n",
    "    start = time()\n",
    "    rnd_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Print metrics \n",
    "    if print_result:\n",
    "        \n",
    "        print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "              \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "        print()\n",
    "        report(rnd_clf.cv_results_)\n",
    "        y_act, y_pred = y_val, clf.predict(X_val)\n",
    "        scores = cross_val_score(clf, X_val, y_val, cv=20, scoring='accuracy')\n",
    "        print(\"Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "        print()\n",
    "        print(classification_report(y_act, y_pred))\n",
    "       \n",
    "    return rnd_clf.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Grid search works in a similar way to random search. The main difference is that it uses a more systematic approach. Instead of a distribution it uses a grid of all the values that should be tested. Then it uses bruteforce to test and validate every possible combination in that grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_hyperparameters(clf, hyper_param, X_train, y_train, X_val, y_val, print_result=False):\n",
    "    \n",
    "    # Run grid search\n",
    "    grid_clf = GridSearchCV(clf, param_grid=hyper_param)\n",
    "    \n",
    "    start = time()\n",
    "    grid_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Print metrics \n",
    "    if print_result:\n",
    "        \n",
    "        print(\"GridSearchCV took %.2f seconds for %d candidates\"\n",
    "              \" parameter settings.\" % ((time() - start), len(grid_clf.cv_results_[\"rank_test_score\"])))\n",
    "        print()\n",
    "        report(grid_clf.cv_results_)\n",
    "        y_act, y_pred = y_val, clf.predict(X_val)\n",
    "        scores = cross_val_score(clf, X_val, y_val, cv=20, scoring='accuracy')\n",
    "        print(\"Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "        print()\n",
    "        print(classification_report(y_act, y_pred))\n",
    "    \n",
    "    return grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing The Models\n",
    "Hyperparameter tuning is where computing can start be a bit more heavy even for conventional machine learning algorithms. I found that the best results in my earlier predictions on the test set were usually made by Random Forest or SVM models. From now on I will focus on them.\n",
    "\n",
    "The method of finding the best parameters will be to start with wide ranges which then can be narrowed down with increased resolution. The initial values will be values that I have seen other people use. For grid search a good way of finding values is to increase the parameter with a factor ~3 for each step, i.e. [1, 3, 10, 30, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 60.35 seconds for 100 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.832 (std: 0.015)\n",
      "Parameters: {'max_depth': 14, 'min_samples_leaf': 10, 'min_samples_split': 14, 'bootstrap': False, 'criterion': 'gini', 'max_features': 4}\n",
      "\n",
      "Validation Accuracy: 0.84 (+/- 0.10)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.84      0.84       113\n",
      "          1       0.72      0.72      0.72        65\n",
      "\n",
      "avg / total       0.80      0.80      0.80       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "hyper_param = {\"max_depth\": stats.randint(3, X_train.shape[1]),\n",
    "               \"max_features\": stats.randint(3, X_train.shape[1]),\n",
    "               \"min_samples_split\": stats.randint(2, 21),\n",
    "               \"min_samples_leaf\": stats.randint(1, 21),\n",
    "               \"bootstrap\": [True, False],\n",
    "               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rf_rnd_clf = random_search_hyperparameters(rf_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 125.42 seconds for 216 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.836 (std: 0.013)\n",
      "Parameters: {'max_depth': 3, 'min_samples_leaf': 3, 'min_samples_split': 10, 'bootstrap': True, 'criterion': 'gini', 'max_features': 10}\n",
      "\n",
      "Validation Accuracy: 0.83 (+/- 0.09)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.84      0.84       113\n",
      "          1       0.72      0.72      0.72        65\n",
      "\n",
      "avg / total       0.80      0.80      0.80       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "hyper_param = {\"max_depth\": [3, None],\n",
    "               \"max_features\": [3, 10, X_train.shape[1]],\n",
    "               \"min_samples_split\": [2, 3, 10],\n",
    "               \"min_samples_leaf\": [1, 3, 10],\n",
    "               \"bootstrap\": [True, False],\n",
    "               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rf_grid_clf = grid_search_hyperparameters(rf_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 37.21 seconds for 100 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.812 (std: 0.020)\n",
      "Parameters: {'gamma': 0.2960297267584936, 'kernel': 'rbf', 'C': 190}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.812 (std: 0.013)\n",
      "Parameters: {'gamma': 0.014153272163606623, 'kernel': 'rbf', 'C': 758}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.812 (std: 0.021)\n",
      "Parameters: {'gamma': 0.526831795447228, 'kernel': 'rbf', 'C': 45}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.812 (std: 0.021)\n",
      "Parameters: {'gamma': 0.6881663274146442, 'kernel': 'rbf', 'C': 52}\n",
      "\n",
      "Validation Accuracy: 0.81 (+/- 0.10)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.88      0.87       113\n",
      "          1       0.79      0.75      0.77        65\n",
      "\n",
      "avg / total       0.84      0.84      0.84       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "hyper_param = {'C': stats.randint(10, 1000),\n",
    "               'gamma' : stats.uniform(0.001, 1),\n",
    "               'kernel': ['rbf']}\n",
    "\n",
    "svm_rnd_clf = random_search_hyperparameters(svm_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 24.94 seconds for 40 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.832 (std: 0.016)\n",
      "Parameters: {'gamma': 0.1, 'kernel': 'rbf', 'C': 1}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.832 (std: 0.016)\n",
      "Parameters: {'gamma': 0.01, 'kernel': 'rbf', 'C': 100}\n",
      "\n",
      "Validation Accuracy: 0.81 (+/- 0.10)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.88      0.87       113\n",
      "          1       0.79      0.75      0.77        65\n",
      "\n",
      "avg / total       0.84      0.84      0.84       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "hyper_param = {'C': [1, 3, 10, 100, 300, 1000, 3000, 10000], \n",
    "               'gamma' : [0.001,0.003, 0.01, 0.1, 1], \n",
    "               'kernel': ['rbf']}\n",
    "\n",
    "svm_grid_clf = grid_search_hyperparameters(svm_clf, hyper_param, X_train, y_train, X_val, y_val, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Final Predictions\n",
    "Now we have completed all the steps necessary to make our final predictions. Looking at the output of our parameter searches we seem that the results seem to be similar for both random and grid search for both algorithms. They basically do the same things, and which one to use is mostly a matter of preference even though there is some debate about the subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "df_test = predict(df_test, rf_rnd_clf, 'C:/GitHub/kaggle/titanic/predictions/predictions_tuned_random_forest.csv')\n",
    "df_test = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "df_test = predict(df_test, svm_grid_clf, 'C:/GitHub/kaggle/titanic/predictions/predictions_tuned_svm.csv')\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "Neural networks are perhaps not the solution you would use for this problem unless it's because they are cool. Primarily it lacks the volumes of data that can make a neural network shine over more conventional alogrithms. But since we think they are cool, and we want to practice, we will implement one anyways.\n",
    "\n",
    "If we wanted to be practical we would stay with scikit-learn and use a [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) which would train a network for us in no time. But instead we will opt for [Tensorflow](https://www.tensorflow.org/) which is a library that is more specialized in custom deep learning solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the model from a parameter dictionary\n",
    "def create_model(paramters):\n",
    "    \n",
    "    num_features = parameters['num_features']\n",
    "    \n",
    "    # Define the size of our data, number of samples can vary and is therefore set to None\n",
    "    X = tf.placeholder(tf.float32, [None, num_features], name='X')\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='y')\n",
    "\n",
    "    layers_dim = paramters['layers_dim']\n",
    "    \n",
    "    # Set up the layer structure of the network\n",
    "    fc = tf.contrib.layers.stack(X, tf.contrib.layers.fully_connected, layers_dim)\n",
    "    \n",
    "    # Make an output layer, no activation function since it will be handled be loss funciton\n",
    "    Z = tf.contrib.layers.fully_connected(fc, 1, activation_fn=None, scope='Z')\n",
    "    \n",
    "    # Define a sigmoid based discrete classifier as a loss function\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z, labels=y, name='Loss')\n",
    "    \n",
    "    # Define a function to measure cost\n",
    "    cost = tf.reduce_mean(loss, name='Cost')\n",
    "    \n",
    "    # Use Adams as a optimization algorithm and tell it to minimize cost\n",
    "    learning_rate = parameters['learning_rate']\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate, name='AdamsOptimizer').minimize(cost)\n",
    "    \n",
    "    # Prediction utils\n",
    "    prediction = tf.round(tf.sigmoid(Z))\n",
    "    correct_prediction = tf.equal(prediction, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Build into a dictionary\n",
    "    model = {'X': X, 'y': y, 'Z': Z, 'cost': cost,\n",
    "             'train_op': train_op, 'prediction': prediction, 'accuracy': accuracy}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split training set into batches\n",
    "def random_mini_batches(X_train, Y_train, mini_batch_size = 32, seed = 0):\n",
    " \n",
    "    # Shuffle with identical seed to get same shuffle in both X and Y\n",
    "    np.random.seed(seed)\n",
    "    X_train = np.random.permutation(X_train)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    Y_train = np.random.permutation(Y_train)\n",
    "    \n",
    "    m = X_train.shape[0]\n",
    "    num_batches = int(m / mini_batch_size)\n",
    "    \n",
    "    # Split data into smaller batches for ready for stochastic gradient descent\n",
    "    minibatches_X = np.array_split(X_train, num_batches)\n",
    "    minibatches_Y = np.array_split(Y_train, num_batches)\n",
    "    \n",
    "    minibatches = zip(minibatches_X, minibatches_Y)\n",
    "    \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(parameters, model):\n",
    "    \n",
    "    num_epochs = parameters['num_epochs']\n",
    "    minibatch_size = parameters['minibatch_size']\n",
    "    X_train = parameters['X_train']\n",
    "    y_train = parameters['y_train']    \n",
    "    train_size = X_train.shape[0]\n",
    "    \n",
    "    # Saver to store the model on disc after training\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    epoch_list = []\n",
    "    cost_list = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Initialize Tensorflow variables\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Run training epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0.\n",
    "            \n",
    "            # Split the training set into random batches\n",
    "            num_minibatches = int(train_size / minibatch_size)\n",
    "            minibatches = random_mini_batches(X_train, y_train, minibatch_size)\n",
    "            \n",
    "            # Run all batches of training data\n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                # Deconstruct and reformat batch data \n",
    "                (minibatch_X, minibatch_y) = minibatch\n",
    "                minibatch_y = np.reshape(minibatch_y, (minibatch_X.shape[0], 1))\n",
    "                \n",
    "                # Build variables dictionary to feed the Tensorflow session with\n",
    "                feed_dict = {model['X'] : minibatch_X, model['y'] : minibatch_y}\n",
    "                \n",
    "                # Run the model in a tensor flow session\n",
    "                _model ,minibatch_cost = sess.run([model['train_op'], model['cost']], feed_dict= feed_dict)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "            if parameters['print'] and (epoch % parameters['print_freq'] == 0):\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            \n",
    "            if parameters['save_cost'] and (epoch % parameters['save_cost_freq'] == 0):\n",
    "                epoch_list.append(epoch)\n",
    "                cost_list.append(epoch_cost)\n",
    "        \n",
    "        # Store model on disc\n",
    "        saver.save(sess, parameters['save_path'])\n",
    "        \n",
    "    return {'epoch_list': epoch_list, 'cost_list' : cost_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "\n",
    "# set model parameters\n",
    "parameters['X_train'] = X_train\n",
    "parameters['y_train'] = y_train\n",
    "parameters['X_val'] = X_val\n",
    "parameters['y_val'] = y_val\n",
    "parameters['X_test'] = pd.read_csv('C:/GitHub/kaggle/titanic/data/test.csv', sep=',', header=0)\n",
    "parameters['PassangerId'] = pd.DataFrame(parameters['X_test']['PassengerId'], columns=['PassengerId'])\n",
    "parameters['layers_dim'] = [14]\n",
    "parameters['num_features'] = X_train.shape[1]\n",
    "parameters['layers_dim'] = [14]\n",
    "parameters['learning_rate'] = 0.01\n",
    "\n",
    "# set train parameters (hyper parameter)\n",
    "parameters['num_epochs'] = 3000\n",
    "parameters['minibatch_size'] = 20\n",
    "\n",
    "# set option parameters\n",
    "parameters['model_name'] = 'nn_clf'\n",
    "parameters['save_path'] = 'C:/GitHub/kaggle/titanic/models/' + parameters['model_name']\n",
    "parameters['print'] = True\n",
    "parameters['print_freq'] = 500\n",
    "parameters['save_cost'] = True\n",
    "parameters['save_cost_freq'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.531866\n",
      "Cost after epoch 500: 0.336066\n",
      "Cost after epoch 1000: 0.330772\n",
      "Cost after epoch 1500: 0.324859\n",
      "Cost after epoch 2000: 0.321741\n",
      "Cost after epoch 2500: 0.318820\n"
     ]
    }
   ],
   "source": [
    "# Train a model and save it to disc\n",
    "with tf.Graph().as_default():\n",
    "    model = create_model(parameters)\n",
    "    plot_data = train_model(parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpJhFPwiIhlGwS8ikmH+P3Icl1f4cFdUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b9037c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print\n",
    "if parameters['save_cost']:\n",
    "    plt.plot(plot_data['epoch_list'], plot_data['cost_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(parameters, model):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Initialize TF variables\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Load model from disc\n",
    "        saver.restore(sess, parameters['save_path'])\n",
    "        \n",
    "        # Make evaluation\n",
    "        print (\"Train Accuracy:\", model['accuracy'].eval({model['X']: X_train, \n",
    "                                                          model['y']: np.reshape(y_train, (X_train.shape[0], 1)) }))\n",
    "        print (\"Validation Accuracy:\", model['accuracy'].eval({model['X']: X_val, \n",
    "                                                          model['y']: np.reshape(y_val, (X_val.shape[0], 1)) }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/GitHub/kaggle/titanic/models/nn_clf\n",
      "Train Accuracy: 0.87798035\n",
      "Validation Accuracy: 0.8426966\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a model that is loaded from disc\n",
    "with tf.Graph().as_default():\n",
    "    model = create_model(parameters)\n",
    "    evaluate(parameters, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction With Neural Networks\n",
    "The difference in accuracy between training and validation implies that our network is overfitting the training data. We could use some regularization techniques such as dropout, but we will skip past this. We move straight to testing our model since I don't have much faith in that we will achieve the highest score here no matter what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "def predict_nn(parameters, model):\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    X_test = parameters['X_test']\n",
    "    X_test = preprocess_dataframe(X_test, prediction_data=True, print_info=False)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, parameters['save_path'])\n",
    "        prediction = model['prediction'].eval({model['X']: X_test})\n",
    "        return prediction.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/GitHub/kaggle/titanic/models/nn_clf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "\n",
    "df_pred = parameters['PassangerId']\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    model = create_model(parameters)\n",
    "    y_test = predict_nn(parameters, model)\n",
    "    df_test['Survived'] = y_test\n",
    "    df_test.to_csv('C:/GitHub/kaggle/titanic/predictions/predictions_neural_network.csv', sep=',', index=False)\n",
    "\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "I have gained some wisedoms from working on this problem. Mainly I have realized how competitive conventional models can be. With scikit-learn it's so easy to test a bunch of them out and get models up and running quickly. To put it simply, don't use deep learning unless you have to.... or because it is fun! :)\n",
    "\n",
    "Another insight, which I already had, but was reinforced here, was the importance of data. It was where most of the performance was improved. I have found that feature engineering is not an exact science but also seem to be an art form where there is no \"correct\" approach.\n",
    "\n",
    "My test result was 80%, which I feel is a good score. However, when you perform a lot of validation on your test set then you increase the risk of getting a model that is biased towards that test set. Ideally you would like to be able to use fresh set of data to know the true accuracy of your model. So I would not be surprised if one of the lower scoring models end up doing better on the other part of the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
